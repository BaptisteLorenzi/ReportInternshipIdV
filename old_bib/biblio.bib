
@article{abbott_mind_2020,
  title    = {The {Mind} of a {Mouse}},
  volume   = {182},
  issn     = {0092-8674},
  url      = {https://www.sciencedirect.com/science/article/pii/S0092867420310011},
  doi      = {10.1016/j.cell.2020.08.010},
  abstract = {Large scientific projects in genomics and astronomy are influential not because they answer any single question but because they enable investigation of continuously arising new questions from the same data-rich sources. Advances in automated mapping of the brain’s synaptic connections (connectomics) suggest that the complicated circuits underlying brain function are ripe for analysis. We discuss benefits of mapping a mouse brain at the level of synapses.},
  language = {en},
  number   = {6},
  urldate  = {2023-06-29},
  journal  = {Cell},
  author   = {Abbott, Larry F. and Bock, Davi D. and Callaway, Edward M. and Denk, Winfried and Dulac, Catherine and Fairhall, Adrienne L. and Fiete, Ila and Harris, Kristen M. and Helmstaedter, Moritz and Jain, Viren and Kasthuri, Narayanan and LeCun, Yann and Lichtman, Jeff W. and Littlewood, Peter B. and Luo, Liqun and Maunsell, John H. R. and Reid, R. Clay and Rosen, Bruce R. and Rubin, Gerald M. and Sejnowski, Terrence J. and Seung, H. Sebastian and Svoboda, Karel and Tank, David W. and Tsao, Doris and Van Essen, David C.},
  month    = sep,
  year     = {2020},
  keywords = {read, connectom, cortex, whole brain, mouse},
  pages    = {1372--1376},
  file     = {ScienceDirect Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/BH55MRDR/Abbott et al. - 2020 - The Mind of a Mouse.pdf:application/pdf;ScienceDirect Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/IUDLZXTZ/S0092867420310011.html:text/html}
}

@article{baccus_fast_2002,
  title    = {Fast and slow contrast adaptation in retinal circuitry},
  volume   = {36},
  issn     = {0896-6273},
  doi      = {10.1016/s0896-6273(02)01050-4},
  abstract = {The visual system adapts to the magnitude of intensity fluctuations, and this process begins in the retina. Following the switch from a low-contrast environment to one of high contrast, ganglion cell sensitivity declines in two distinct phases: a fast change occurs in {\textless}0.1 s, and a slow decrease over approximately 10 s. To examine where these modulations arise, we recorded intracellularly from every major cell type in the salamander retina. Certain bipolar and amacrine cells, and all ganglion cells, adapted to contrast. Generally, these neurons showed both fast and slow adaptation. Fast effects of a contrast increase included accelerated kinetics, decreased sensitivity, and a depolarization of the baseline membrane potential. Slow adaptation did not affect kinetics, but produced a gradual hyperpolarization. This hyperpolarization can account for slow adaptation in the spiking output of ganglion cells.},
  language = {eng},
  number   = {5},
  journal  = {Neuron},
  author   = {Baccus, Stephen A. and Meister, Markus},
  month    = dec,
  year     = {2002},
  pmid     = {12467594},
  keywords = {Retina, Neurons, Animals, Membrane Potentials, Urodela, Adaptation, read, Adaptation, Ocular, Contrast Sensitivity, Electrophysiology, Models, Biological, Photic Stimulation, Rabbits, Time Factors},
  pages    = {909--919},
  file     = {Texte intégral:/home/baptistel/snap/zotero-snap/common/Zotero/storage/ZKFB53Z3/Baccus et Meister - 2002 - Fast and slow contrast adaptation in retinal circu.pdf:application/pdf}
}

@article{baden_functional_2016,
  title     = {The functional diversity of retinal ganglion cells in the mouse},
  volume    = {529},
  copyright = {2016 Springer Nature Limited},
  issn      = {1476-4687},
  url       = {https://www.nature.com/articles/nature16468},
  doi       = {10.1038/nature16468},
  abstract  = {In the vertebrate visual system, all output of the retina is carried by retinal ganglion cells. Each type encodes distinct visual features in parallel for transmission to the brain. How many such ‘output channels’ exist and what each encodes are areas of intense debate. In the mouse, anatomical estimates range from 15 to 20 channels, and only a handful are functionally understood. By combining two-photon calcium imaging to obtain dense retinal recordings and unsupervised clustering of the resulting sample of more than 11,000 cells, here we show that the mouse retina harbours substantially more than 30 functional output channels. These include all known and several new ganglion cell types, as verified by genetic and anatomical criteria. Therefore, information channels from the mouse eye to the mouse brain are considerably more diverse than shown thus far by anatomical studies, suggesting an encoding strategy resembling that used in state-of-the-art artificial vision systems.},
  language  = {en},
  number    = {7586},
  urldate   = {2023-07-19},
  journal   = {Nature},
  author    = {Baden, Tom and Berens, Philipp and Franke, Katrin and Román Rosón, Miroslav and Bethge, Matthias and Euler, Thomas},
  month     = jan,
  year      = {2016},
  note      = {Number: 7586
               Publisher: Nature Publishing Group},
  keywords  = {Neural encoding, Retina, Sensory processing, priority:0},
  pages     = {345--350},
  file      = {Accepted Version:/home/baptistel/snap/zotero-snap/common/Zotero/storage/KL8EYM2X/Baden et al. - 2016 - The functional diversity of retinal ganglion cells.pdf:application/pdf}
}

@article{baker_mechanistic_2018,
  title    = {Mechanistic models versus machine learning, a fight worth fighting for the biological community?},
  volume   = {14},
  url      = {https://royalsocietypublishing.org/doi/10.1098/rsbl.2017.0660},
  doi      = {10.1098/rsbl.2017.0660},
  abstract = {Ninety per cent of the world's data have been generated in the last 5 years (Machine learning: the power and promise of computers that learn by example. Report no. DES4702. Issued April 2017. Royal Society). A small fraction of these data is collected with the aim of validating specific hypotheses. These studies are led by the development of mechanistic models focused on the causality of input–output relationships. However, the vast majority is aimed at supporting statistical or correlation studies that bypass the need for causality and focus exclusively on prediction. Along these lines, there has been a vast increase in the use of machine learning models, in particular in the biomedical and clinical sciences, to try and keep pace with the rate of data generation. Recent successes now beg the question of whether mechanistic models are still relevant in this area. Said otherwise, why should we try to understand the mechanisms of disease progression when we can use machine learning tools to directly predict disease outcome?},
  number   = {5},
  urldate  = {2023-07-11},
  journal  = {Biology Letters},
  author   = {Baker, Ruth E. and Peña, Jose-Maria and Jayamohan, Jayaratnam and Jérusalem, Antoine},
  month    = may,
  year     = {2018},
  note     = {Publisher: Royal Society},
  keywords = {read, machine learning, mechanistic modelling, quantitative biology},
  pages    = {20170660},
  file     = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/7GQLMQYE/Baker et al. - 2018 - Mechanistic models versus machine learning, a figh.pdf:application/pdf}
}

@article{bargmann_connectome_2013,
  title     = {From the connectome to brain function},
  volume    = {10},
  copyright = {2013 Springer Nature America, Inc.},
  issn      = {1548-7105},
  url       = {https://www.nature.com/articles/nmeth.2451},
  doi       = {10.1038/nmeth.2451},
  abstract  = {In this Historical Perspective, we ask what information is needed beyond connectivity diagrams to understand the function of nervous systems. Informed by invertebrate circuits whose connectivities are known, we highlight the importance of neuronal dynamics and neuromodulation, and the existence of parallel circuits. The vertebrate retina has these features in common with invertebrate circuits, suggesting that they are general across animals. Comparisons across these systems suggest approaches to study the functional organization of large circuits based on existing knowledge of small circuits.},
  language  = {en},
  number    = {6},
  urldate   = {2023-07-11},
  journal   = {Nature Methods},
  author    = {Bargmann, Cornelia I. and Marder, Eve},
  month     = jun,
  year      = {2013},
  note      = {Number: 6
               Publisher: Nature Publishing Group},
  keywords  = {functional, priority:1, Neuroscience, Neurophysiology, connectomic},
  pages     = {483--490}
}

@article{barlow_mechanism_1965,
  title    = {The mechanism of directionally selective units in rabbit's retina},
  volume   = {178},
  issn     = {0022-3751},
  doi      = {10.1113/jphysiol.1965.sp007638},
  language = {eng},
  number   = {3},
  journal  = {The Journal of Physiology},
  author   = {Barlow, H. B. and Levick, W. R.},
  month    = jun,
  year     = {1965},
  pmid     = {5827909},
  pmcid    = {PMC1357309},
  keywords = {Retina, Animals, Rabbits, Motion Perception, Sensory Receptor Cells},
  pages    = {477--504},
  file     = {Full Text:/home/baptistel/snap/zotero-snap/common/Zotero/storage/ZMQX8QKN/Barlow and Levick - 1965 - The mechanism of directionally selective units in .pdf:application/pdf}
}

@article{ben-ari_gabaa_1997,
  title      = {{GABAA}, {NMDA} and {AMPA} receptors: a developmentally regulated 'ménage à trois'},
  volume     = {20},
  issn       = {0166-2236},
  shorttitle = {{GABAA}, {NMDA} and {AMPA} receptors},
  doi        = {10.1016/s0166-2236(97)01147-8},
  abstract   = {The main ionotropic receptors (GABAA, NMDA and AMPA) display a sequential participation in neuronal excitation in the neonatal hippocampus. GABA, the principal inhibitory transmitter in the adult CNS, acts as an excitatory transmitter in early postnatal stage. Glutamatergic synaptic transmission is first purely NMDA-receptor based and lacks functional AMPA receptors. Therefore, initially glutamatergic synapses are 'silent' at resting membrane potential, NMDA channels being blocked by Mg2+. However, when GABA and glutamatergic synapses are coactivated during the physiological patterns of activity, GABAA receptors can facilitate the activation of NMDA receptors, playing the role conferred to AMPA receptors later on in development. Determining the mechanisms underlying the development of this 'ménage à trois' will shed light not only on the wide range of trophic roles of glutamate and GABA in the developing brain, but also on the significance of the transition from neonatal to adult forms of plasticity.},
  language   = {eng},
  number     = {11},
  journal    = {Trends in Neurosciences},
  author     = {Ben-Ari, Y. and Khazipov, R. and Leinekugel, X. and Caillard, O. and Gaiarsa, J. L.},
  month      = nov,
  year       = {1997},
  pmid       = {9364667},
  keywords   = {Neurons, Animals, Synapse, Hippocampus, Humans, Receptors, AMPA, Receptors, GABA-A, Receptors, N-Methyl-D-Aspartate},
  pages      = {523--529},
  file       = {Full Text:/home/baptistel/snap/zotero-snap/common/Zotero/storage/SEQMXNYM/Ben-Ari et al. - 1997 - GABAA, NMDA and AMPA receptors a developmentally .pdf:application/pdf}
}

@article{betsch_world_2004,
  title    = {The world from a cat's perspective - {Statistics} of natural videos},
  volume   = {90},
  doi      = {10.1007/s00422-003-0434-6},
  abstract = {The mammalian visual system is one of the most intensively investigated sensory systems. However, our knowledge of the typical input it is operating on is surprisingly limited. To address this issue, we seek to learn about the natural visual environment and the world as seen by a cat. With a CCD camera attached to their head, cats explore several outdoor environments and videos of natural stimuli are recorded from the animals' perspective. The statistical analysis of these videos reveals several remarkable properties. First, we find an anisotropy of oriented contours with an enhanced occurrence of horizontal orientations, earlier described in the "oblique effect" as a predominance of the two cardinal orientations. Second, contrast is not elevated in the center of the images, suggesting different mechanisms of fixation point selection as compared to humans. Third, analyzing a sequence of images we find that the precise position of contours varies faster than their orientation. Finally, collinear contours prevail over parallel shifted contours, matching recent physiological and anatomical results. These findings demonstrate the rich structure of natural visual stimuli and its direct relation to extensively studied anatomical and physiological issues.},
  journal  = {Biological cybernetics},
  author   = {Betsch, Belinda and Einhäuser, Wolfgang and Kording, Konrad and König, Peter},
  month    = feb,
  year     = {2004},
  pages    = {41--50},
  file     = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/PGFAI3BN/Betsch et al. - 2004 - The world from a cat's perspective - Statistics of.pdf:application/pdf}
}

@techreport{blanco_malerba_jointly_2023,
  type        = {preprint},
  title       = {Jointly efficient encoding and decoding in neural populations},
  url         = {http://biorxiv.org/lookup/doi/10.1101/2023.06.21.545848},
  abstract    = {The efficient coding approach proposes that neural systems represent as much sensory information as biological constraints allow. It aims at formalizing encoding as a constrained optimal process. A different approach, that aims at formalizing decoding, proposes that neural systems instantiate a generative model of the sensory world. Here, we put forth a normative framework that characterizes neural systems as jointly optimizing encoding and decoding. It takes the form of a variational autoencoder: sensory stimuli are encoded in the noisy activity of neurons to be interpreted by a flexible decoder; encoding must allow for an accurate stimulus reconstruction from neural activity. Jointly, neural activity is required to represent the statistics of latent features which are mapped by the decoder into distributions over sensory stimuli; decoding correspondingly optimizes the accuracy of the generative model. This framework results in a family of encoding-decoding models, which result in equally accurate generative models, indexed by a measure of the stimulus-induced deviation of neural activity from the prior distribution over neural activity. Each member of this family predicts a specific relation between properties of the sensory neurons—such as the arrangement of the tuning curve means (preferred stimuli) and widths (degrees of selectivity) in the population—as a function of the statistics of the sensory world. Our approach thus generalizes the efficient coding approach. Notably, here, the form of the constraint on the optimization derives from the requirement of an accurate generative model, while it is arbitrary in efficient coding models. Finally, we characterize the family of models we obtain through other measures of performance, such as the error in stimulus reconstruction. We find that a range of models admit comparable performance; in particular, a population of sensory neurons with broad tuning curves as observed experimentally yields both low reconstruction stimulus error and an accurate generative model.},
  language    = {en},
  urldate     = {2023-06-27},
  institution = {Neuroscience},
  author      = {Blanco Malerba, Simone and Micheli, Aurora and Woodford, Michael and Azeredo Da Silveira, Rava},
  month       = jun,
  year        = {2023},
  doi         = {10.1101/2023.06.21.545848},
  keywords    = {Priority:0, IDV, decoding, coding},
  file        = {Blanco Malerba et al. - 2023 - Jointly efficient encoding and decoding in neural .pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/6FZV6UXS/Blanco Malerba et al. - 2023 - Jointly efficient encoding and decoding in neural .pdf:application/pdf}
}

@article{boerlin_predictive_2013,
  title    = {Predictive {Coding} of {Dynamical} {Variables} in {Balanced} {Spiking} {Networks}},
  volume   = {9},
  issn     = {1553-7358},
  url      = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003258},
  doi      = {10.1371/journal.pcbi.1003258},
  abstract = {Two observations about the cortex have puzzled neuroscientists for a long time. First, neural responses are highly variable. Second, the level of excitation and inhibition received by each neuron is tightly balanced at all times. Here, we demonstrate that both properties are necessary consequences of neural networks that represent information efficiently in their spikes. We illustrate this insight with spiking networks that represent dynamical variables. Our approach is based on two assumptions: We assume that information about dynamical variables can be read out linearly from neural spike trains, and we assume that neurons only fire a spike if that improves the representation of the dynamical variables. Based on these assumptions, we derive a network of leaky integrate-and-fire neurons that is able to implement arbitrary linear dynamical systems. We show that the membrane voltage of the neurons is equivalent to a prediction error about a common population-level signal. Among other things, our approach allows us to construct an integrator network of spiking neurons that is robust against many perturbations. Most importantly, neural variability in our networks cannot be equated to noise. Despite exhibiting the same single unit properties as widely used population code models (e.g. tuning curves, Poisson distributed spike trains), balanced networks are orders of magnitudes more reliable. Our approach suggests that spikes do matter when considering how the brain computes, and that the reliability of cortical representations could have been strongly underestimated.},
  language = {en},
  number   = {11},
  urldate  = {2023-05-05},
  journal  = {PLOS Computational Biology},
  author   = {Boerlin, Martin and Machens, Christian K. and Denève, Sophie},
  month    = nov,
  year     = {2013},
  note     = {Publisher: Public Library of Science},
  keywords = {Membrane potential, Neurons, Coding, Priority:2, Action potentials, Dynamical systems, Network analysis, Neural networks, Neuronal tuning, Sensory perception},
  pages    = {e1003258},
  file     = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/BB475V4B/Boerlin et al. - 2013 - Predictive Coding of Dynamical Variables in Balanc.pdf:application/pdf}
}

@article{bonin_statistical_2006,
  title     = {The {Statistical} {Computation} {Underlying} {Contrast} {Gain} {Control}},
  volume    = {26},
  copyright = {Copyright © 2006 Society for Neuroscience 0270-6474/06/266346-08\$15.00/0},
  issn      = {0270-6474, 1529-2401},
  url       = {https://www.jneurosci.org/content/26/23/6346},
  doi       = {10.1523/JNEUROSCI.0284-06.2006},
  abstract  = {In the early visual system, a contrast gain control mechanism sets the gain of responses based on the locally prevalent contrast. The measure of contrast used by this adaptation mechanism is commonly assumed to be the standard deviation of light intensities relative to the mean (root-mean-square contrast). A number of alternatives, however, are possible. For example, the measure of contrast might depend on the absolute deviations relative to the mean, or on the prevalence of the darkest or lightest intensities. We investigated the statistical computation underlying this measure of contrast in the cat's lateral geniculate nucleus, which relays signals from retina to cortex. Borrowing a method from psychophysics, we recorded responses to white noise stimuli whose distribution of intensities was precisely varied. We varied the standard deviation, skewness, and kurtosis of the distribution of intensities while keeping the mean luminance constant. We found that gain strongly depends on the standard deviation of the distribution. At constant standard deviation, moreover, gain is invariant to changes in skewness or kurtosis. These findings held for both ON and OFF cells, indicating that the measure of contrast is independent of the range of stimulus intensities signaled by the cells. These results confirm the long-held assumption that contrast gain control computes root-mean-square contrast. They also show that contrast gain control senses the full distribution of intensities and leaves unvaried the relative responses of the different cell types. The advantages to visual processing of this remarkably specific computation are not entirely known.},
  language  = {en},
  number    = {23},
  urldate   = {2023-07-10},
  journal   = {Journal of Neuroscience},
  author    = {Bonin, Vincent and Mante, Valerio and Carandini, Matteo},
  month     = jun,
  year      = {2006},
  pmid      = {16763043},
  note      = {Publisher: Society for Neuroscience
               Section: Articles},
  keywords  = {adaptation, Gain Control, inhibition, retina, priority:1, receptive field, image statistics, luminance, texture},
  pages     = {6346--6353},
  file      = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/MSXJMVAY/Bonin et al. - 2006 - The Statistical Computation Underlying Contrast Ga.pdf:application/pdf}
}

@article{borst_adaptation_nodate,
  title    = {Adaptation without parameter change: {Dynamic} gain control in motion detection {\textbar} {PNAS}},
  url      = {https://www.pnas.org/doi/10.1073/pnas.0500491102},
  urldate  = {2023-05-26},
  author   = {Borst, Alexander},
  keywords = {Adaptation, Cortex, Gain control, read},
  file     = {Adaptation without parameter change\: Dynamic gain control in motion detection | PNAS:/home/baptistel/snap/zotero-snap/common/Zotero/storage/GLJMKX2U/pnas.html:text/html;Borst - Adaptation without parameter change Dynamic gain .pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/GP2PXW94/Borst - Adaptation without parameter change Dynamic gain .pdf:application/pdf}
}

@article{cadena_deep_2019,
  title    = {Deep convolutional models improve predictions of macaque {V1} responses to natural images},
  volume   = {15},
  issn     = {1553-7358},
  url      = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006897},
  doi      = {10.1371/journal.pcbi.1006897},
  abstract = {Despite great efforts over several decades, our best models of primary visual cortex (V1) still predict spiking activity quite poorly when probed with natural stimuli, highlighting our limited understanding of the nonlinear computations in V1. Recently, two approaches based on deep learning have emerged for modeling these nonlinear computations: transfer learning from artificial neural networks trained on object recognition and data-driven convolutional neural network models trained end-to-end on large populations of neurons. Here, we test the ability of both approaches to predict spiking activity in response to natural images in V1 of awake monkeys. We found that the transfer learning approach performed similarly well to the data-driven approach and both outperformed classical linear-nonlinear and wavelet-based feature representations that build on existing theories of V1. Notably, transfer learning using a pre-trained feature space required substantially less experimental time to achieve the same performance. In conclusion, multi-layer convolutional neural networks (CNNs) set the new state of the art for predicting neural responses to natural images in primate V1 and deep features learned for object recognition are better explanations for V1 computation than all previous filter bank theories. This finding strengthens the necessity of V1 models that are multiple nonlinearities away from the image domain and it supports the idea of explaining early visual cortex based on high-level functional goals.},
  language = {en},
  number   = {4},
  urldate  = {2023-05-02},
  journal  = {PLOS Computational Biology},
  author   = {Cadena, Santiago A. and Denfield, George H. and Walker, Edgar Y. and Gatys, Leon A. and Tolias, Andreas S. and Bethge, Matthias and Ecker, Alexander S.},
  year     = {2019},
  note     = {Publisher: Public Library of Science},
  keywords = {Neurons, read, CNN, DL, Neural networks, Neuronal tuning, Aspect ratio, Machine learning, Monkeys, Vision, Visual cortex},
  pages    = {e1006897},
  file     = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/S8A3TUX8/Cadena et al. - 2019 - Deep convolutional models improve predictions of m.pdf:application/pdf}
}

@article{chance_gain_2002,
  title    = {Gain modulation from background synaptic input},
  volume   = {35},
  issn     = {0896-6273},
  doi      = {10.1016/s0896-6273(02)00820-6},
  abstract = {Gain modulation is a prominent feature of neuronal activity recorded in behaving animals, but the mechanism by which it occurs is unknown. By introducing a barrage of excitatory and inhibitory synaptic conductances that mimics conditions encountered in vivo into pyramidal neurons in slices of rat somatosensory cortex, we show that the gain of a neuronal response to excitatory drive can be modulated by varying the level of "background" synaptic input. Simultaneously increasing both excitatory and inhibitory background firing rates in a balanced manner results in a divisive gain modulation of the neuronal response without appreciable signal-independent increases in firing rate or spike-train variability. These results suggest that, within active cortical circuits, the overall level of synaptic input to a neuron acts as a gain control signal that modulates responsiveness to excitatory drive.},
  language = {eng},
  number   = {4},
  journal  = {Neuron},
  author   = {Chance, Frances S. and Abbott, L. F. and Reyes, Alex D.},
  month    = aug,
  year     = {2002},
  pmid     = {12194875},
  keywords = {Neurons, Animals, Models, Neurological, Action Potentials, Artifacts, Electric Stimulation, Genetic Variation, Neural Inhibition, Neural Pathways, Organ Culture Techniques, Rats, Somatosensory Cortex, Synapses, Synaptic Transmission},
  pages    = {773--782},
  file     = {Texte intégral:/home/baptistel/snap/zotero-snap/common/Zotero/storage/GP26LHCY/Chance et al. - 2002 - Gain modulation from background synaptic input.pdf:application/pdf}
}

@article{chen_alert_2013,
  title    = {Alert response to motion onset in the retina},
  volume   = {33},
  issn     = {1529-2401},
  doi      = {10.1523/JNEUROSCI.3749-12.2013},
  abstract = {Previous studies have shown that motion onset is very effective at capturing attention and is more salient than smooth motion. Here, we find that this salience ranking is present already in the firing rate of retinal ganglion cells. By stimulating the retina with a bar that appears, stays still, and then starts moving, we demonstrate that a subset of salamander retinal ganglion cells, fast OFF cells, responds significantly more strongly to motion onset than to smooth motion. We refer to this phenomenon as an alert response to motion onset. We develop a computational model that predicts the time-varying firing rate of ganglion cells responding to the appearance, onset, and smooth motion of a bar. This model, termed the adaptive cascade model, consists of a ganglion cell that receives input from a layer of bipolar cells, represented by individual rectified subunits. Additionally, both the bipolar and ganglion cells have separate contrast gain control mechanisms. This model captured the responses to our different motion stimuli over a wide range of contrasts, speeds, and locations. The alert response to motion onset, together with its computational model, introduces a new mechanism of sophisticated motion processing that occurs early in the visual system.},
  language = {eng},
  number   = {1},
  journal  = {The Journal of Neuroscience: The Official Journal of the Society for Neuroscience},
  author   = {Chen, Eric Y. and Marre, Olivier and Fisher, Clark and Schwartz, Greg and Levy, Joshua and da Silveira, Rava Azeredo and Berry, Michael J.},
  month    = jan,
  year     = {2013},
  pmid     = {23283327},
  pmcid    = {PMC3711149},
  keywords = {Retina, Animals, Retinal Ganglion Cells, read, Action Potentials, Motion Perception, Ambystoma, Attention, Gain Control, Motion},
  pages    = {120--132},
  file     = {Full Text:/home/baptistel/snap/zotero-snap/common/Zotero/storage/7MK3RRV2/Chen et al. - 2013 - Alert response to motion onset in the retina.pdf:application/pdf}
}

@article{ching_opportunities_2018,
  title    = {Opportunities and obstacles for deep learning in biology and medicine},
  volume   = {15},
  number   = {141},
  journal  = {Journal of The Royal Society Interface},
  author   = {Ching, Travers and Himmelstein, Daniel S. and Beaulieu-Jones, Brett K. and Kalinin, Alexandr A. and Do, Brian T. and Way, Gregory P. and Ferrero, Enrico and Agapow, Paul-Michael and Zietz, Michael and Hoffman, Michael M.},
  year     = {2018},
  note     = {Publisher: The Royal Society},
  keywords = {priority:0},
  pages    = {20170387},
  file     = {Ching et al. - 2018 - Opportunities and obstacles for deep learning in b.pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/YDID3LPY/Ching et al. - 2018 - Opportunities and obstacles for deep learning in b.pdf:application/pdf}
}

@article{demb_functional_2015,
  title    = {Functional {Circuitry} of the {Retina}},
  volume   = {1},
  issn     = {2374-4650},
  doi      = {10.1146/annurev-vision-082114-035334},
  abstract = {The mammalian retina is an important model system for studying neural circuitry: Its role in sensation is clear, its cell types are relatively well defined, and its responses to natural stimuli-light patterns-can be studied in vitro. To solve the retina, we need to understand how the circuits presynaptic to its output neurons, ganglion cells, divide the visual scene into parallel representations to be assembled and interpreted by the brain. This requires identifying the component interneurons and understanding how their intrinsic properties and synapses generate circuit behaviors. Because the cellular composition and fundamental properties of the retina are shared across species, basic mechanisms studied in the genetically modifiable mouse retina apply to primate vision. We propose that the apparent complexity of retinal computation derives from a straightforward mechanism-a dynamic balance of synaptic excitation and inhibition regulated by use-dependent synaptic depression-applied differentially to the parallel pathways that feed ganglion cells.},
  language = {eng},
  journal  = {Annual Review of Vision Science},
  author   = {Demb, Jonathan B. and Singer, Joshua H.},
  month    = nov,
  year     = {2015},
  pmid     = {28532365},
  pmcid    = {PMC5749398},
  keywords = {read, adaptation, amacrine cell, Pathway, bipolar cell, ganglion cell, parallel pathways, synaptic depression},
  pages    = {263--289},
  file     = {mouseVision.jpg:/home/lorenzib/Documents/mouseVision.jpg:image/jpeg;Version acceptée:/home/baptistel/snap/zotero-snap/common/Zotero/storage/RL57SIQA/Demb et Singer - 2015 - Functional Circuitry of the Retina.pdf:application/pdf}
}

@article{deny_multiplexed_2017,
  title     = {Multiplexed computations in retinal ganglion cells of a single type},
  volume    = {8},
  copyright = {2017 The Author(s)},
  issn      = {2041-1723},
  url       = {https://www.nature.com/articles/s41467-017-02159-y},
  doi       = {10.1038/s41467-017-02159-y},
  abstract  = {In the early visual system, cells of the same type perform the same computation in different places of the visual field. How these cells code together a complex visual scene is unclear. A common assumption is that cells of a single-type extract a single-stimulus feature to form a feature map, but this has rarely been observed directly. Using large-scale recordings in the rat retina, we show that a homogeneous population of fast OFF ganglion cells simultaneously encodes two radically different features of a visual scene. Cells close to a moving object code quasilinearly for its position, while distant cells remain largely invariant to the object’s position and, instead, respond nonlinearly to changes in the object’s speed. We develop a quantitative model that accounts for this effect and identify a disinhibitory circuit that mediates it. Ganglion cells of a single type thus do not code for one, but two features simultaneously. This richer, flexible neural map might also be present in other sensory systems.},
  language  = {en},
  number    = {1},
  urldate   = {2023-07-19},
  journal   = {Nature Communications},
  author    = {Deny, Stéphane and Ferrari, Ulisse and Macé, Emilie and Yger, Pierre and Caplette, Romain and Picaud, Serge and Tkačik, Gašper and Marre, Olivier},
  month     = dec,
  year      = {2017},
  note      = {Number: 1
               Publisher: Nature Publishing Group},
  keywords  = {Neural encoding, Retina, Priority:0, IDV, Network models},
  pages     = {1964},
  file      = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/9675CEL9/Deny et al. - 2017 - Multiplexed computations in retinal ganglion cells.pdf:application/pdf}
}

@article{diamond_inhibitory_2017,
  title      = {Inhibitory {Interneurons} in the {Retina}: {Types}, {Circuitry}, and {Function}},
  volume     = {3},
  shorttitle = {Inhibitory {Interneurons} in the {Retina}},
  url        = {https://doi.org/10.1146/annurev-vision-102016-061345},
  doi        = {10.1146/annurev-vision-102016-061345},
  abstract   = {Visual signals in the vertebrate retina are shaped by feedback and feedforward inhibition in two synaptic layers. In one, horizontal cells establish fundamental center-surround receptive-field properties via morphologically and physiologically complex synapses with photoreceptors and bipolar cells. In the other, a panoply of amacrine cells imbue ganglion cell responses with spatiotemporally complex information about the visual world. Here, I review current ideas about horizontal cell signaling, considering the evidence for and against the leading, competing theories. I also discuss recent work that has begun to make sense of the remarkable morphological and physiological diversity of amacrine cells. These latter efforts have been aided tremendously by increasingly complete connectivity maps of inner retinal circuitry and new genetic tools that enable study of individual, sparsely expressed amacrine cell types.},
  number     = {1},
  urldate    = {2023-06-01},
  journal    = {Annual Review of Vision Science},
  author     = {Diamond, Jeffrey S.},
  year       = {2017},
  pmid       = {28617659},
  note       = {\_eprint: https://doi.org/10.1146/annurev-vision-102016-061345},
  keywords   = {read, inhibition, amacrine cell, feedback inhibition, feedforward inhibition, horizontal cell, Pathway, visual processing},
  pages      = {1--24},
  file       = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/T8U3F9YI/Diamond - 2017 - Inhibitory Interneurons in the Retina Types, Circ.pdf:application/pdf}
}

@article{dicarlo_how_2012,
  title    = {How does the brain solve visual object recognition?},
  volume   = {73},
  issn     = {0896-6273},
  url      = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3306444/},
  doi      = {10.1016/j.neuron.2012.01.010},
  abstract = {Mounting evidence suggests that “core object recognition,” the ability to rapidly recognize objects despite substantial appearance variation, is solved in the brain via a cascade of reflexive, largely feedforward computations that culminate in a powerful neuronal representation in the inferior temporal cortex. However, the algorithm that produces this solution remains little-understood. Here we review evidence ranging from individual neurons, to neuronal populations, to behavior, to computational models. We propose that understanding this algorithm will require using neuronal and psychophysical data to sift through many computational models, each based on building blocks of small, canonical sub-networks with a common functional goal.},
  number   = {3},
  urldate  = {2023-05-22},
  journal  = {Neuron},
  author   = {DiCarlo, James J. and Zoccolan, Davide and Rust, Nicole C.},
  month    = feb,
  year     = {2012},
  pmid     = {22325196},
  pmcid    = {PMC3306444},
  keywords = {Priority:2, Brain},
  pages    = {415--434},
  file     = {PubMed Central Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/XWR5HCQT/DiCarlo et al. - 2012 - How does the brain solve visual object recognition.pdf:application/pdf}
}

@misc{ding_information_2023,
  title     = {Information {Geometry} of the {Retinal} {Representation} {Manifold}},
  copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  url       = {https://www.biorxiv.org/content/10.1101/2023.05.17.541206v1},
  doi       = {10.1101/2023.05.17.541206},
  abstract  = {The ability to discriminate visual stimuli is constrained by their retinal representations. Previous studies of visual discriminability were limited to either low-dimensional artificial stimuli or theoretical considerations without a realistic model. Here we propose a novel framework for understanding stimulus discriminability achieved by retinal representations of naturalistic stimuli with the method of information geometry. To model the joint probability distribution of neural responses conditioned on the stimulus, we created a stochastic encoding model of a population of salamander retinal ganglion cells based on a three-layer convolutional neural network model. This model not only accurately captured the mean response to natural scenes but also a variety of second-order statistics. With the model and the proposed theory, we are able to compute the Fisher information metric over stimuli and study the most discriminable stimulus directions. We found that the most discriminable stimulus varied substantially, allowing an examination of the relationship between the most discriminable stimulus and the current stimulus. We found that the most discriminative response mode is often aligned with the most stochastic mode. This finding carries the important implication that under natural scenes noise correlations in the retina are information-limiting rather than aiding in increasing information transmission as has been previously speculated. We observed that sensitivity saturates less in the population than for single cells and also that Fisher information varies less than sensitivity as a function of firing rate. We conclude that under natural scenes, population coding benefits from complementary coding and helps to equalize the information carried by different firing rates, which may facilitate decoding of the stimulus under principles of information maximization.},
  language  = {en},
  urldate   = {2023-07-24},
  publisher = {bioRxiv},
  author    = {Ding, Xuehao and Lee, Dongsoo and Melander, Josh B. and Sivulka, George and Ganguli, Surya and Baccus, Stephen A.},
  month     = may,
  year      = {2023},
  note      = {Pages: 2023.05.17.541206
               Section: New Results},
  keywords  = {read, information theory, DL, noise correlation},
  file      = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/TTNZR9VZ/Ding et al. - 2023 - Information Geometry of the Retinal Representation.pdf:application/pdf}
}

@article{dong_commentary_2018,
  title      = {Commentary: {Using} goal-driven deep learning models to understand sensory cortex},
  volume     = {12},
  issn       = {1662-5188},
  shorttitle = {Commentary},
  doi        = {10.3389/fncom.2018.00004},
  language   = {eng},
  journal    = {Frontiers in Computational Neuroscience},
  author     = {Dong, Qiulei and Wang, Hong and Hu, Zhanyi},
  year       = {2018},
  pmid       = {29403369},
  pmcid      = {PMC5780340},
  keywords   = {categorization, convergent features, goal-driven deep learning models, hierarchical convolutional neural network, IT neuron},
  pages      = {4},
  file       = {Full Text:/home/baptistel/snap/zotero-snap/common/Zotero/storage/YJM5UCUE/Dong et al. - 2018 - Commentary Using goal-driven deep learning models.pdf:application/pdf}
}

@article{doya_social_2022,
  title    = {Social impact and governance of {AI} and neurotechnologies},
  volume   = {152},
  issn     = {0893-6080},
  url      = {https://www.sciencedirect.com/science/article/pii/S0893608022001861},
  doi      = {10.1016/j.neunet.2022.05.012},
  abstract = {Advances in artificial intelligence (AI) and brain science are going to have a huge impact on society. While technologies based on those advances can provide enormous social benefits, adoption of new technologies poses various risks. This article first reviews the co-evolution of AI and brain science and the benefits of brain-inspired AI in sustainability, healthcare, and scientific discoveries. We then consider possible risks from those technologies, including intentional abuse, autonomous weapons, cognitive enhancement by brain–computer interfaces, insidious effects of social media, inequity, and enfeeblement. We also discuss practical ways to bring ethical principles into practice. One proposal is to stop giving explicit goals to AI agents and to enable them to keep learning human preferences. Another is to learn from democratic mechanisms that evolved in human society to avoid over-consolidation of power. Finally, we emphasize the importance of open discussions not only by experts, but also including a diverse array of lay opinions.},
  language = {en},
  urldate  = {2023-05-22},
  journal  = {Neural Networks},
  author   = {Doya, Kenji and Ema, Arisa and Kitano, Hiroaki and Sakagami, Masamichi and Russell, Stuart},
  month    = aug,
  year     = {2022},
  keywords = {DL, AI, AI scientist, Artificial intelligence, Ethics, Governance, Human compatible AI, Neurotechnology, Social},
  pages    = {542--554},
  file     = {ScienceDirect Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/M8GFWEDC/S0893608022001861.html:text/html;Texte intégral:/home/baptistel/snap/zotero-snap/common/Zotero/storage/35Q74NWI/Doya et al. - 2022 - Social impact and governance of AI and neurotechno.pdf:application/pdf}
}

@misc{ebert_temporal_2023,
  title     = {Temporal pattern recognition in retinal ganglion cells is mediated by dynamical inhibitory synapses},
  copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  url       = {https://www.biorxiv.org/content/10.1101/2023.01.12.523643v1},
  doi       = {10.1101/2023.01.12.523643},
  abstract  = {A fundamental task for the brain is to generate predictions of future sensory inputs, and signal errors in these predictions. Many neurons have been shown to signal omitted stimuli during periodic stimulation, even in the retina. However, the mechanisms of this error signaling are unclear. Here we show that depressing inhibitory synapses enable the retina to signal an omitted stimulus in a flash sequence. While ganglion cells, the retinal output, responded to an omitted flash with a constant latency over many frequencies of the flash sequence, we found that this was not the case once inhibition was blocked. We built a simple circuit model and showed that depressing inhibitory synapses were a necessary component to reproduce our experimental findings. We also generated new predictions with this model, that we confirmed experimentally. Depressing inhibitory synapses could thus be a key component to generate the predictive responses observed in many brain areas.},
  language  = {en},
  urldate   = {2023-02-03},
  publisher = {bioRxiv},
  author    = {Ebert, Simone and Buffet, Thomas and Sermet, B. Semihcan and Marre, Olivier and Cessac, Bruno},
  month     = jan,
  year      = {2023},
  note      = {Pages: 2023.01.12.523643
               Section: New Results},
  keywords  = {Priority:0, IDV},
  file      = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/8HTMKSKT/Ebert et al. - 2023 - Temporal pattern recognition in retinal ganglion c.pdf:application/pdf}
}

@article{ferrari_simple_2018,
  title    = {A {Simple} {Model} for {Low} {Variability} in {Neural} {Spike} {Trains}},
  volume   = {30},
  issn     = {0899-7667, 1530-888X},
  url      = {https://direct.mit.edu/neco/article/30/11/3009-3036/8421},
  doi      = {10.1162/neco_a_01125},
  abstract = {Neural noise sets a limit to information transmission in sensory systems. In several areas, the spiking response (to a repeated stimulus) has shown a higher degree of regularity than predicted by a Poisson process. However, a simple model to explain this low variability is still lacking. Here we introduce a new model, with a correction to Poisson statistics, that can accurately predict the regularity of neural spike trains in response to a repeated stimulus. The model has only two parameters but can reproduce the observed variability in retinal recordings in various conditions. We show analytically why this approximation can work. In a model of the spike-emitting process where a refractory period is assumed, we derive that our simple correction can well approximate the spike train statistics over a broad range of firing rates. Our model can be easily plugged to stimulus processing models, like a linear-nonlinear model or its generalizations, to replace the Poisson spike train hypothesis that is commonly assumed. It estimates the amount of information transmitted much more accurately than Poisson models in retinal recordings. Thanks to its simplicity, this model has the potential to explain low variability in other areas.},
  language = {en},
  number   = {11},
  urldate  = {2023-05-15},
  journal  = {Neural Computation},
  author   = {Ferrari, Ulisse and Deny, Stéphane and Marre, Olivier and Mora, Thierry},
  month    = nov,
  year     = {2018},
  keywords = {IDV},
  pages    = {3009--3036},
  file     = {Ferrari et al. - 2018 - A Simple Model for Low Variability in Neural Spike.pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/FZLVCJX6/Ferrari et al. - 2018 - A Simple Model for Low Variability in Neural Spike.pdf:application/pdf}
}

@article{garvert_local_2013,
  title    = {Local and global contrast adaptation in retinal ganglion cells},
  volume   = {77},
  issn     = {1097-4199},
  doi      = {10.1016/j.neuron.2012.12.030},
  abstract = {Retinal ganglion cells react to changes in visual contrast by adjusting their sensitivity and temporal filtering characteristics. This contrast adaptation has primarily been studied under spatially homogeneous stimulation. Yet, ganglion cell receptive fields are often characterized by spatial subfields, providing a substrate for nonlinear spatial processing. This raises the question whether contrast adaptation follows a similar subfield structure or whether it occurs globally over the receptive field even for local stimulation. We therefore recorded ganglion cell activity in isolated salamander retinas while locally changing visual contrast. Ganglion cells showed primarily global adaptation characteristics, with notable exceptions in certain aspects of temporal filtering. Surprisingly, some changes in filtering were most pronounced for locations where contrast did not change. This seemingly paradoxical effect can be explained by a simple computational model, which emphasizes the importance of local nonlinearities in the retina and suggests a reevaluation of previously reported local contrast adaptation.},
  language = {eng},
  number   = {5},
  journal  = {Neuron},
  author   = {Garvert, Mona M. and Gollisch, Tim},
  month    = mar,
  year     = {2013},
  pmid     = {23473321},
  keywords = {Animals, Models, Neurological, Nonlinear Dynamics, Retinal Ganglion Cells, Visual Fields, read, Adaptation, Physiological, Computer Simulation, Contrast Sensitivity, Photic Stimulation, adaptation, Ambystoma mexicanum, Electrophysiological Phenomena, Extracellular Space},
  pages    = {915--928},
  file     = {Texte intégral:/home/baptistel/snap/zotero-snap/common/Zotero/storage/7VT84RHI/Garvert et Gollisch - 2013 - Local and global contrast adaptation in retinal ga.pdf:application/pdf}
}

@article{goetz_unified_2022,
  title    = {Unified classification of mouse retinal ganglion cells using function, morphology, and gene expression},
  volume   = {40},
  issn     = {2211-1247},
  doi      = {10.1016/j.celrep.2022.111040},
  abstract = {Classification and characterization of neuronal types are critical for understanding their function and dysfunction. Neuronal classification schemes typically rely on measurements of electrophysiological, morphological, and molecular features, but aligning such datasets has been challenging. Here, we present a unified classification of mouse retinal ganglion cells (RGCs), the sole retinal output neurons. We use visually evoked responses to classify 1,859 mouse RGCs into 42 types. We also obtain morphological or transcriptomic data from subsets and use these measurements to align the functional classification to publicly available morphological and transcriptomic datasets. We create an online database that allows users to browse or download the data and to classify RGCs from their light responses using a machine learning algorithm. This work provides a resource for studies of RGCs, their upstream circuits in the retina, and their projections in the brain, and establishes a framework for future efforts in neuronal classification and open data distribution.},
  language = {eng},
  number   = {2},
  journal  = {Cell Reports},
  author   = {Goetz, Jillian and Jessen, Zachary F. and Jacobi, Anne and Mani, Adam and Cooler, Sam and Greer, Devon and Kadri, Sabah and Segal, Jeremy and Shekhar, Karthik and Sanes, Joshua R. and Schwartz, Gregory W.},
  month    = jul,
  year     = {2022},
  pmid     = {35830791},
  pmcid    = {PMC9364428},
  keywords = {Retina, Animals, Retinal Ganglion Cells, read, Mice, CP: Neuroscience, Gene Expression, retina, retinal ganglion cell, transcriptomics, morphology, light responses, classification, typing, AdaBoost},
  pages    = {111040},
  file     = {Full Text:/home/baptistel/snap/zotero-snap/common/Zotero/storage/N5KHHGHM/Goetz et al. - 2022 - Unified classification of mouse retinal ganglion c.pdf:application/pdf;UnifiedClassSuppMat.pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/2Q9IDNVM/UnifiedClassSuppMat.pdf:application/pdf}
}

@article{goldin_context-dependent_2022,
  title     = {Context-dependent selectivity to natural images in the retina},
  volume    = {13},
  copyright = {2022 The Author(s)},
  issn      = {2041-1723},
  url       = {https://www.nature.com/articles/s41467-022-33242-8},
  doi       = {10.1038/s41467-022-33242-8},
  abstract  = {Retina ganglion cells extract specific features from natural scenes and send this information to the brain. In particular, they respond to local light increase (ON responses), and/or decrease (OFF). However, it is unclear if this ON-OFF selectivity, characterized with synthetic stimuli, is maintained under natural scene stimulation. Here we recorded ganglion cell responses to natural images slightly perturbed by random noise patterns to determine their selectivity during natural stimulation. The ON-OFF selectivity strongly depended on the specific image. A single ganglion cell can signal luminance increase for one image, and luminance decrease for another. Modeling and experiments showed that this resulted from the non-linear combination of different retinal pathways. Despite the versatility of the ON-OFF selectivity, a systematic analysis demonstrated that contrast was reliably encoded in these responses. Our perturbative approach uncovered the selectivity of retinal ganglion cells to more complex features than initially thought.},
  language  = {en},
  number    = {1},
  urldate   = {2023-05-02},
  journal   = {Nature Communications},
  author    = {Goldin, Matías A. and Lefebvre, Baptiste and Virgili, Samuele and Pham Van Cang, Mathieu Kim and Ecker, Alexander and Mora, Thierry and Ferrari, Ulisse and Marre, Olivier},
  month     = sep,
  year      = {2022},
  note      = {Number: 1
               Publisher: Nature Publishing Group},
  keywords  = {Neural encoding, Retina, read, IDV},
  pages     = {5556},
  file      = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/5M46VRKT/Goldin et al. - 2022 - Context-dependent selectivity to natural images in.pdf:application/pdf;ReviewGoldin2022.pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/PF9HTLN3/ReviewGoldin2022.pdf:application/pdf;SupplementaryMatGoldin2022.pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/HQDYVCNL/SupplementaryMatGoldin2022.pdf:application/pdf}
}

@misc{goldin_scalable_2023,
  title     = {Scalable gaussian process inference of neural responses to natural images},
  copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  url       = {https://www.biorxiv.org/content/10.1101/2023.01.13.523423v1},
  doi       = {10.1101/2023.01.13.523423},
  abstract  = {1 Abstract
               Predicting the responses of sensory neurons is a long-standing neuroscience goal. However, while there has been much progress in modeling neural responses to simple and/or artificial stimuli, predicting responses to natural stimuli remains an ongoing challenge. One the one hand, deep neural networks perform very well on certain data-sets, but can fail when data is limited. On the other hand, gaussian processes (GPs) perform well on limited data, but are generally poor at predicting responses to high-dimensional stimuli, such as natural images. Here we show how structured priors, e.g. for local and smooth receptive fields, can be used to scale up GPs to high-dimensional stimuli. We show that when we do this, a GP model largely outperforms a deep neural network trained to predict retinal responses to natural images, with largest differences observed when both models are trained on a very small data-set. Further, since GPs compute the uncertainty in their predictions, they are well-suited to closed-loop experiments, where stimuli are chosen actively so as to collect ‘informative’ neural data. We show how this can be done in practice on our retinal data-set, so as to: (i) efficiently learn a model of retinal responses to natural images, using little data, and (ii) rapidly distinguish between competing models (e.g. a linear vs a non-linear model). In the future, our approach could be applied to other low-level sensory areas, beyond the retina.},
  language  = {en},
  urldate   = {2023-05-04},
  publisher = {bioRxiv},
  author    = {Goldin, Matías A. and Virgili, Samuele and Chalk, Matthew},
  month     = jan,
  year      = {2023},
  note      = {Pages: 2023.01.13.523423
               Section: New Results},
  keywords  = {read, GP},
  file      = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/IM254PIS/Goldin et al. - 2023 - Scalable gaussian process inference of neural resp.pdf:application/pdf}
}

@article{gollisch_eye_2010,
  title      = {Eye smarter than scientists believed: {Neural} computations in circuits of the retina},
  volume     = {65},
  issn       = {0896-6273},
  shorttitle = {Eye smarter than scientists believed},
  url        = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3717333/},
  doi        = {10.1016/j.neuron.2009.12.009},
  abstract   = {We rely on our visual system to cope with the vast barrage of incoming light patterns and to extract features from the scene that are relevant to our well-being. The necessary reduction of visual information already begins in the eye. In this review, we summarize recent progress in understanding the computations performed in the vertebrate retina and how they are implemented by the neural circuitry. A new picture emerges from these findings that helps resolve a vexing paradox between the retina’s structure and function. Whereas the conventional wisdom treats the eye as a simple pre-filter for visual images, it now appears that the retina solves a diverse set of specific tasks, and provides the results explicitly to downstream brain areas.},
  number     = {2},
  urldate    = {2023-05-15},
  journal    = {Neuron},
  author     = {Gollisch, Tim and Meister, Markus},
  month      = jan,
  year       = {2010},
  pmid       = {20152123},
  pmcid      = {PMC3717333},
  keywords   = {read, functional, pathways, printed},
  pages      = {150--164},
  file       = {gollisch2010.pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/WDFZTD28/gollisch2010.pdf:application/pdf;PubMed Central Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/BJZRALBZ/Gollisch and Meister - 2010 - Eye smarter than scientists believed Neural compu.pdf:application/pdf}
}

@article{greschner_identification_2016,
  title    = {Identification of a {Retinal} {Circuit} for {Recurrent} {Suppression} {Using} {Indirect} {Electrical} {Imaging}},
  volume   = {26},
  issn     = {0960-9822},
  url      = {https://www.sciencedirect.com/science/article/pii/S0960982216305486},
  doi      = {10.1016/j.cub.2016.05.051},
  abstract = {Understanding the function of modulatory interneuron networks is a major challenge, because such networks typically operate over long spatial scales and involve many neurons of different types. Here, we use an indirect electrical imaging method to reveal the function of a spatially extended, recurrent retinal circuit composed of two cell types. This recurrent circuit produces peripheral response suppression of early visual signals in the primate magnocellular visual pathway. We identify a type of polyaxonal amacrine cell physiologically via its distinctive electrical signature, revealed by electrical coupling with ON parasol retinal ganglion cells recorded using a large-scale multi-electrode array. Coupling causes the amacrine cells to fire spikes that propagate radially over long distances, producing GABA-ergic inhibition of other ON parasol cells recorded near the amacrine cell axonal projections. We propose and test a model for the function of this amacrine cell type, in which the extra-classical receptive field of ON parasol cells is formed by reciprocal inhibition from other ON parasol cells in the periphery, via the electrically coupled amacrine cell network.},
  language = {en},
  number   = {15},
  urldate  = {2023-05-05},
  journal  = {Current Biology},
  author   = {Greschner, Martin and Heitman, Alexander K. and Field, Greg D. and Li, Peter H. and Ahn, Daniel and Sher, Alexander and Litke, Alan M. and Chichilnisky, E. J.},
  month    = aug,
  year     = {2016},
  keywords = {Priority:2, inhibition},
  pages    = {1935--1942},
  file     = {ScienceDirect Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/VS74XKMW/Greschner et al. - 2016 - Identification of a Retinal Circuit for Recurrent .pdf:application/pdf;ScienceDirect Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/BL5L6DXQ/S0960982216305486.html:text/html}
}

@article{guerguiev_towards_2017,
  title    = {Towards deep learning with segregated dendrites},
  volume   = {6},
  doi      = {10.7554/eLife.22901},
  abstract = {Deep learning has led to significant advances in artificial intelligence, in part, by adopting strategies motivated by neurophysiology. However, it is unclear whether deep learning could occur in the real brain. Here, we show that a deep learning algorithm that utilizes multi-compartment neurons might help us to understand how the neocortex optimizes cost functions. Like neocortical pyramidal neurons, neurons in our model receive sensory information and higher-order feedback in electrotonically segregated compartments. Thanks to this segregation, neurons in different layers of the network can coordinate synaptic weight updates. As a result, the network learns to categorize images better than a single layer network. Furthermore, we show that our algorithm takes advantage of multilayer architectures to identify useful higher-order representations-the hallmark of deep learning. This work demonstrates that deep learning can be achieved using segregated dendritic compartments, which may help to explain the morphology of neocortical pyramidal neurons.},
  journal  = {eLife},
  author   = {Guerguiev, Jordan and Lillicrap, Timothy and Richards, Blake},
  month    = dec,
  year     = {2017},
  file     = {Full Text:/home/baptistel/snap/zotero-snap/common/Zotero/storage/9GPJIH4A/Guerguiev et al. - 2017 - Towards deep learning with segregated dendrites.pdf:application/pdf}
}

@article{hateren_processing_2002,
  title     = {Processing of {Natural} {Temporal} {Stimuli} by {Macaque} {Retinal} {Ganglion} {Cells}},
  volume    = {22},
  copyright = {Copyright © 2002 Society for Neuroscience},
  issn      = {0270-6474, 1529-2401},
  url       = {https://www.jneurosci.org/content/22/22/9945},
  doi       = {10.1523/JNEUROSCI.22-22-09945.2002},
  abstract  = {This study quantifies the performance of primate retinal ganglion cells in response to natural stimuli. Stimuli were confined to the temporal and chromatic domains and were derived from two contrasting environments, one typically northern European and the other a flower show. The performance of the cells was evaluated by investigating variability of cell responses to repeated stimulus presentations and by comparing measured to model responses. Both analyses yielded a quantity called the coherence rate (in bits per second), which is related to the information rate. Magnocellular (MC) cells yielded coherence rates of up to 100 bits/sec, rates of parvocellular (PC) cells were much lower, and short wavelength (S)-cone-driven ganglion cells yielded intermediate rates. The modeling approach showed that for MC cells, coherence rates were generated almost exclusively by the luminance content of the stimulus. Coherence rates of PC cells were also dominated by achromatic content. This is a consequence of the stimulus structure; luminance varied much more in the natural environment than chromaticity. Only approximately one-sixth of the coherence rate of the PC cells derived from chromatic content, and it was dominated by frequencies below 10 Hz. S-cone-driven ganglion cells also yielded coherence rates dominated by low frequencies. Below 2–3 Hz, PC cell signals contained more power than those of MC cells. Response variation between individual ganglion cells of a particular class was analyzed by constructing generic cells, the properties of which may be relevant for performance higher in the visual system. The approach used here helps define retinal modules useful for studies of higher visual processing of natural stimuli.},
  language  = {en},
  number    = {22},
  urldate   = {2023-06-14},
  journal   = {Journal of Neuroscience},
  author    = {Hateren, J. H. van and Rüttiger, L. and Sun, H. and Lee, B. B.},
  month     = nov,
  year      = {2002},
  pmid      = {12427852},
  note      = {Publisher: Society for Neuroscience
               Section: ARTICLE},
  keywords  = {natural stimuli, information theory, macaque, magnocellular, parvocellular, retinal ganglion cells},
  pages     = {9945--9960},
  file      = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/NSKNW2L4/Hateren et al. - 2002 - Processing of Natural Temporal Stimuli by Macaque .pdf:application/pdf}
}

@article{helmstaedter_connectomic_2013,
  title    = {Connectomic reconstruction of the inner plexiform layer in the mouse retina},
  volume   = {500},
  number   = {7461},
  journal  = {Nature},
  author   = {Helmstaedter, Moritz and Briggman, Kevin L. and Turaga, Srinivas C. and Jain, Viren and Seung, H. Sebastian and Denk, Winfried},
  year     = {2013},
  note     = {Publisher: Nature Publishing Group UK London},
  keywords = {read, ML, connectomic},
  pages    = {168--174},
  file     = {Helmstaedter et al. - 2013 - Connectomic reconstruction of the inner plexiform .pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/2927KT4E/Helmstaedter et al. - 2013 - Connectomic reconstruction of the inner plexiform .pdf:application/pdf}
}

@misc{hofling_chromatic_2022,
  title     = {A chromatic feature detector in the retina signals visual context changes},
  copyright = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  url       = {https://www.biorxiv.org/content/10.1101/2022.11.30.518492v1},
  doi       = {10.1101/2022.11.30.518492},
  abstract  = {The retina transforms patterns of light into visual feature representations supporting behaviour. These representations are distributed across various types of retinal ganglion cells (RGCs), whose spatial and temporal tuning properties have been extensively studied in many model organisms, including the mouse. However, it has been difficult to link the potentially nonlinear retinal transformations of natural visual inputs to specific ethological purposes. Here, we discover a novel selectivity to chromatic contrast in an RGC type that allows the detection of transitions of the horizon across a retinal region. We trained a convolutional neural network (CNN) model on large-scale functional recordings of RGC responses to natural mouse movies, and then used this model to search in silico for stimuli that maximally excite distinct types of RGCs. This procedure predicted centre colour-opponency in transient Suppressed-by-Contrast RGCs (tSbC), a cell type whose function is being debated. We confirmed experimentally that these cells indeed responded very selectively to Green-OFF, UV-ON contrasts, which we found to be characteristic of transitions from ground to sky in the visual scene, as might be elicited by head-or eye-movements across the horizon. Because tSbCs reliably detected these transitions, we suggest a role for this RGC type in providing contextual information (i.e. sky or ground) necessary for the selection of appropriate behavioural responses to other stimuli, such as looming objects. Our work showcases how a combination of experiments with natural stimuli and computational modelling allows discovering novel types of stimulus selectivity and identifying their potential ethological relevance.},
  language  = {en},
  urldate   = {2023-08-28},
  publisher = {bioRxiv},
  author    = {Höfling, Larissa and Szatko, Klaudia P. and Behrens, Christian and Qiu, Yongrong and Klindt, David A. and Jessen, Zachary and Schwartz, Gregory W. and Bethge, Matthias and Berens, Philipp and Franke, Katrin and Ecker, Alexander S. and Euler, Thomas},
  month     = dec,
  year      = {2022},
  note      = {Pages: 2022.11.30.518492
               Section: New Results},
  keywords  = {read, CNN, DL, Patch, typing, colors, 2Photon, Mouse, Natual Videos},
  file      = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/BMCMA8WK/Höfling et al. - 2022 - A chromatic feature detector in the retina signals.pdf:application/pdf}
}

@article{hosoya_dynamic_2005,
  title     = {Dynamic predictive coding by the retina},
  volume    = {436},
  copyright = {2005 Nature Publishing Group},
  issn      = {1476-4687},
  url       = {https://www.nature.com/articles/nature03689},
  doi       = {10.1038/nature03689},
  abstract  = {Retinal ganglion cells convey the visual image from the eye to the brain. They generally encode local differences in space and changes in time rather than the raw image intensity. This can be seen as a strategy of predictive coding, adapted through evolution to the average image statistics of the natural environment. Yet animals encounter many environments with visual statistics different from the average scene. Here we show that when this happens, the retina adjusts its processing dynamically. The spatio-temporal receptive fields of retinal ganglion cells change after a few seconds in a new environment. The changes are adaptive, in that the new receptive field improves predictive coding under the new image statistics. We show that a network model with plastic synapses can account for the large variety of observed adaptations.},
  language  = {en},
  number    = {7047},
  urldate   = {2023-04-25},
  journal   = {Nature},
  author    = {Hosoya, Toshihiko and Baccus, Stephen A. and Meister, Markus},
  month     = jul,
  year      = {2005},
  note      = {Number: 7047
               Publisher: Nature Publishing Group},
  keywords  = {Coding, Adaptation, Humanities and Social Sciences, multidisciplinary, Science},
  pages     = {71--77},
  file      = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/NW5U6IYJ/Hosoya et al. - 2005 - Dynamic predictive coding by the retina.pdf:application/pdf}
}

@article{howlett_novel_2017,
  title    = {A novel mechanism of cone photoreceptor adaptation},
  volume   = {15},
  issn     = {1545-7885},
  url      = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2001210},
  doi      = {10.1371/journal.pbio.2001210},
  abstract = {An animal’s ability to survive depends on its sensory systems being able to adapt to a wide range of environmental conditions, by maximizing the information extracted and reducing the noise transmitted. The visual system does this by adapting to luminance and contrast. While luminance adaptation can begin at the retinal photoreceptors, contrast adaptation has been shown to start at later stages in the retina. Photoreceptors adapt to changes in luminance over multiple time scales ranging from tens of milliseconds to minutes, with the adaptive changes arising from processes within the phototransduction cascade. Here we show a new form of adaptation in cones that is independent of the phototransduction process. Rather, it is mediated by voltage-gated ion channels in the cone membrane and acts by changing the frequency response of cones such that their responses speed up as the membrane potential modulation depth increases and slow down as the membrane potential modulation depth decreases. This mechanism is effectively activated by high-contrast stimuli dominated by low frequencies such as natural stimuli. However, the more generally used Gaussian white noise stimuli were not effective since they did not modulate the cone membrane potential to the same extent. This new adaptive process had a time constant of less than a second. A critical component of the underlying mechanism is the hyperpolarization-activated current, Ih, as pharmacologically blocking it prevented the long- and mid- wavelength sensitive cone photoreceptors (L- and M-cones) from adapting. Consistent with this, short- wavelength sensitive cone photoreceptors (S-cones) did not show the adaptive response, and we found they also lacked a prominent Ih. The adaptive filtering mechanism identified here improves the information flow by removing higher-frequency noise during lower signal-to-noise ratio conditions, as occurs when contrast levels are low. Although this new adaptive mechanism can be driven by contrast, it is not a contrast adaptation mechanism in its strictest sense, as will be argued in the Discussion.},
  language = {en},
  number   = {4},
  urldate  = {2023-06-21},
  journal  = {PLOS Biology},
  author   = {Howlett, Marcus H. C. and Smith, Robert G. and Kamermans, Maarten},
  month    = apr,
  year     = {2017},
  note     = {Publisher: Public Library of Science},
  keywords = {Frequency response, Light, Luminance, Membrane potential, Neurons, Photoreceptors, Phototransduction, Sine waves},
  pages    = {e2001210},
  file     = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/7TU7FR9N/Howlett et al. - 2017 - A novel mechanism of cone photoreceptor adaptation.pdf:application/pdf}
}

@article{huang_estimating_2021,
  title    = {Estimating smooth and sparse neural receptive fields with a flexible spline basis},
  volume   = {5},
  issn     = {2690-2664},
  url      = {http://arxiv.org/abs/2108.07537},
  doi      = {10.51628/001c.27578},
  abstract = {Spatio-temporal receptive field (STRF) models are frequently used to approximate the computation implemented by a sensory neuron. Typically, such STRFs are assumed to be smooth and sparse. Current state-of-the-art approaches for estimating STRFs based on empirical Bayes are often not computationally efficient in high-dimensional settings, as encountered in sensory neuroscience. Here we pursued an alternative approach and encode prior knowledge for estimation of STRFs by choosing a set of basis functions with the desired properties: natural cubic splines. Our method is computationally efficient and can be easily applied to a wide range of existing models. We compared the performance of spline-based methods to non-spline ones on simulated and experimental data, showing that spline-based methods consistently outperform the non-spline versions.},
  number   = {3},
  urldate  = {2023-06-01},
  journal  = {Neurons, Behavior, Data analysis, and Theory},
  author   = {Huang, Ziwei and Ran, Yanli and Oesterle, Jonathan and Euler, Thomas and Berens, Philipp},
  month    = aug,
  year     = {2021},
  note     = {arXiv:2108.07537 [cs]},
  keywords = {Priority:2, Computer Science - Machine Learning},
  file     = {arXiv Fulltext PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/CDR2QNZ4/Huang et al. - 2021 - Estimating smooth and sparse neural receptive fiel.pdf:application/pdf;arXiv.org Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/K5LNH59H/2108.html:text/html}
}

@misc{international_i_nodate,
  title    = {I asked {ChatGPT} about its carbon footprint, and for now, it's a mystery},
  url      = {https://businessinsider.mx/chatgpt-openai-carbon-footprint-ai-climate-crisis-2023-2/},
  abstract = {The artificial-intelligence tool offered up a rather human answer about its carbon footprint: It depends.},
  language = {es},
  urldate  = {2023-05-15},
  journal  = {Business Insider México {\textbar} Noticias pensadas para ti},
  author   = {International, Business Insider},
  file     = {Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/S7DM3G5U/chatgpt-openai-carbon-footprint-ai-climate-crisis-2023-2.html:text/html}
}

@article{jadzinsky_transformation_2013,
  title    = {Transformation of visual signals by inhibitory interneurons in retinal circuits},
  volume   = {36},
  issn     = {1545-4126},
  doi      = {10.1146/annurev-neuro-062012-170315},
  abstract = {One of the largest mysteries of the brain lies in understanding how higher-level computations are implemented by lower-level operations in neurons and synapses. In particular, in many brain regions inhibitory interneurons represent a diverse class of cells, the individual functional roles of which are unknown. We discuss here how the operations of inhibitory interneurons influence the behavior of a circuit, focusing on recent results in the vertebrate retina. A key role in this understanding is played by a common representation of the visual stimulus that can be applied at different stages. By considering how this stimulus representation changes at each location in the circuit, we can understand how neuron-level operations such as thresholds and inhibition yield circuit-level computations such as how stimulus selectivity and gain are controlled by local and peripheral visual stimuli.},
  language = {eng},
  journal  = {Annual Review of Neuroscience},
  author   = {Jadzinsky, Pablo D. and Baccus, Stephen A.},
  month    = jul,
  year     = {2013},
  pmid     = {23724996},
  keywords = {Retina, Animals, Models, Neurological, Neural Inhibition, Nerve Net, Visual Pathways, inhibition, Interneurons, reading},
  pages    = {403--428},
  file     = {Full Text:/home/baptistel/snap/zotero-snap/common/Zotero/storage/FAQDU3YI/Jadzinsky and Baccus - 2013 - Transformation of visual signals by inhibitory int.pdf:application/pdf}
}

@article{janssen_deep_2022,
  title      = {Deep compartment models: {A} deep learning approach for the reliable prediction of time-series data in pharmacokinetic modeling},
  volume     = {11},
  copyright  = {© 2022 The Authors. CPT: Pharmacometrics \& Systems Pharmacology published by Wiley Periodicals LLC on behalf of American Society for Clinical Pharmacology and Therapeutics.},
  issn       = {2163-8306},
  shorttitle = {Deep compartment models},
  url        = {https://onlinelibrary.wiley.com/doi/abs/10.1002/psp4.12808},
  doi        = {10.1002/psp4.12808},
  abstract   = {Nonlinear mixed effect (NLME) models are the gold standard for the analysis of patient response following drug exposure. However, these types of models are complex and time-consuming to develop. There is great interest in the adoption of machine-learning methods, but most implementations cannot be reliably extrapolated to treatment strategies outside of the training data. In order to solve this problem, we propose the deep compartment model (DCM), a combination of neural networks and ordinary differential equations. Using simulated datasets of different sizes, we show that our model remains accurate when training on small data sets. Furthermore, using a real-world data set of patients with hemophilia A receiving factor VIII concentrate while undergoing surgery, we show that our model more accurately predicts a priori drug concentrations compared to a previous NLME model. In addition, we show that our model correctly describes the changing drug concentration over time. By adopting pharmacokinetic principles, the DCM allows for simulation of different treatment strategies and enables therapeutic drug monitoring.},
  language   = {en},
  number     = {7},
  urldate    = {2023-07-03},
  journal    = {CPT: Pharmacometrics \& Systems Pharmacology},
  author     = {Janssen, Alexander and Leebeek, Frank W. G. and Cnossen, Marjon H. and Mathôt, Ron A. A. and {for the OPTI-CLOT study group and SYMPHONY consortium}},
  year       = {2022},
  note       = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/psp4.12808},
  keywords   = {Compartment, priority:1, Deep Learning},
  pages      = {934--945},
  file       = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/U8ZQB9AP/Janssen et al. - 2022 - Deep compartment models A deep learning approach .pdf:application/pdf;Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/NGK5JMAK/psp4.html:text/html}
}

@article{johnston_retinal_2019,
  title    = {A {Retinal} {Circuit} {Generating} a {Dynamic} {Predictive} {Code} for {Oriented} {Features}},
  volume   = {102},
  issn     = {1097-4199},
  doi      = {10.1016/j.neuron.2019.04.002},
  abstract = {Sensory systems must reduce the transmission of redundant information to function efficiently. One strategy is to continuously adjust the sensitivity of neurons to suppress responses to common features of the input while enhancing responses to new ones. Here we image the excitatory synaptic inputs and outputs of retinal ganglion cells to understand how such dynamic predictive coding is implemented in the analysis of spatial patterns. Synapses of bipolar cells become tuned to orientation through presynaptic inhibition, generating lateral antagonism in the orientation domain. Individual ganglion cells receive excitatory synapses tuned to different orientations, but feedforward inhibition generates a high-pass filter that only transmits the initial activation of these inputs, removing redundancy. These results demonstrate how a dynamic predictive code can be implemented by circuit motifs common to many parts of the brain.},
  language = {eng},
  number   = {6},
  journal  = {Neuron},
  author   = {Johnston, Jamie and Seibel, Sofie-Helene and Darnet, Léa Simone Adele and Renninger, Sabine and Orger, Michael and Lagnado, Leon},
  month    = jun,
  year     = {2019},
  pmid     = {31054873},
  pmcid    = {PMC6591004},
  keywords = {Retina, Animals, Retinal Bipolar Cells, Retinal Ganglion Cells, Vision, Ocular, Neural Inhibition, Synapses, Priority:2, Visual Pathways, Larva, Zebrafish, synapse, Escherichia coli Proteins, Glutamic Acid, Green Fluorescent Proteins, Optical Imaging, orientation, Orientation, Spatial, predictive code, Recombinant Fusion Proteins, retina, Space Perception, tectum, vision, zebrafish},
  pages    = {1211--1222.e3},
  file     = {Texte intégral:/home/baptistel/snap/zotero-snap/common/Zotero/storage/BRR44RQI/Johnston et al. - 2019 - A Retinal Circuit Generating a Dynamic Predictive .pdf:application/pdf}
}

@article{kahneman_before_2011,
  title    = {Before {You} {Make} {That} {Big} {Decision}},
  url      = {http://ir.vnulib.edu.vn/handle/123456789/4136},
  abstract = {When an executive makes a big bet, he or she typically relies on the judgment of a team that has put together a proposal for a strategic course of action. After all, the team will have delved into the pros and cons much more deeply than the executive has time to do. The problem is, biases invariably creep into any team's reasoning-and often dangerously distort its thinking. A team that has fallen in love with its recommendation, for instance, may subconsciously dismiss evidence that contradicts its theories, give far too much weight to one piece of data, or make faulty comparisons to another business case. That's why, with important decisions, executives need to conduct a careful review not only of the content of recommendations but of the recommendation process. To that end, the authors-Kahneman, who won a Nobel Prize in economics for his work on cognitive biases; Lovallo of the University of Sydney; and Sibony of McKinsey-have put together a 12-question checklist intended to unearth and neutralize defects in teams' thinking. These questions help leaders examine whether a team has explored alternatives appropriately, gathered all the right information, and used well-grounded numbers to support its case. They also highlight considerations such as whether the team might be unduly influenced by self-interest, overconfidence, or attachment to past decisions.By using this practical tool, executives will build decision processes over time that reduce the effects of biases and upgrade the quality of decisions their organizations make. The payoffs can be significant: A recent McKinsey study of more than 1,000 business investments, for instance, showed that when companies worked to reduce the effects of bias, they raised their returns on investment by seven percentage points. Executives need to realize that the judgment of even highly experienced, superbly competent managers can be fallible. A disciplined decision-making process, not individual genius, is the key to good strategy. INSETS: Idea in Brief; Improving Decisions Throughout the Organization. [ABSTRACT FROM AUTHOR].},
  language = {en},
  urldate  = {2023-05-24},
  author   = {Kahneman, Daniel and Lovallo, Dan and Sibony, Olivier},
  month    = jun,
  year     = {2011},
  note     = {Accepted: 2011-12-07T07:02:33Z},
  keywords = {WTF},
  file     = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/EQM6D7TY/Kahneman et al. - 2011 - Before You Make That Big Decision.pdf:application/pdf}
}

@article{kar_evidence_2019,
  title     = {Evidence that recurrent circuits are critical to the ventral stream’s execution of core object recognition behavior},
  volume    = {22},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  issn      = {1546-1726},
  url       = {https://www.nature.com/articles/s41593-019-0392-5},
  doi       = {10.1038/s41593-019-0392-5},
  abstract  = {Non-recurrent deep convolutional neural networks (CNNs) are currently the best at modeling core object recognition, a behavior that is supported by the densely recurrent primate ventral stream, culminating in the inferior temporal (IT) cortex. If recurrence is critical to this behavior, then primates should outperform feedforward-only deep CNNs for images that require additional recurrent processing beyond the feedforward IT response. Here we first used behavioral methods to discover hundreds of these ‘challenge’ images. Second, using large-scale electrophysiology, we observed that behaviorally sufficient object identity solutions emerged {\textasciitilde}30 ms later in the IT cortex for challenge images compared with primate performance-matched ‘control’ images. Third, these behaviorally critical late-phase IT response patterns were poorly predicted by feedforward deep CNN activations. Notably, very-deep CNNs and shallower recurrent CNNs better predicted these late IT responses, suggesting that there is a functional equivalence between additional nonlinear transformations and recurrence. Beyond arguing that recurrent circuits are critical for rapid object identification, our results provide strong constraints for future recurrent model development.},
  language  = {en},
  number    = {6},
  urldate   = {2023-05-26},
  journal   = {Nature Neuroscience},
  author    = {Kar, Kohitij and Kubilius, Jonas and Schmidt, Kailyn and Issa, Elias B. and DiCarlo, James J.},
  month     = jun,
  year      = {2019},
  note      = {Number: 6
               Publisher: Nature Publishing Group},
  keywords  = {Neural encoding, CNN, DL, Neural decoding, Object vision, recurrence},
  pages     = {974--983},
  file      = {Accepted Version:/home/baptistel/snap/zotero-snap/common/Zotero/storage/L6QTHQGY/Kar et al. - 2019 - Evidence that recurrent circuits are critical to t.pdf:application/pdf}
}

@article{karamanlis_retinal_2022,
  title    = {Retinal {Encoding} of {Natural} {Scenes}},
  volume   = {8},
  issn     = {2374-4650},
  doi      = {10.1146/annurev-vision-100820-114239},
  abstract = {An ultimate goal in retina science is to understand how the neural circuit of the retina processes natural visual scenes. Yet most studies in laboratories have long been performed with simple, artificial visual stimuli such as full-field illumination, spots of light, or gratings. The underlying assumption is that the features of the retina thus identified carry over to the more complex scenario of natural scenes. As the application of corresponding natural settings is becoming more commonplace in experimental investigations, this assumption is being put to the test and opportunities arise to discover processing features that are triggered by specific aspects of natural scenes. Here, we review how natural stimuli have been used to probe, refine, and complement knowledge accumulated under simplified stimuli, and we discuss challenges and opportunities along the way toward a comprehensive understanding of the encoding of natural scenes.},
  language = {eng},
  journal  = {Annual Review of Vision Science},
  author   = {Karamanlis, Dimokratis and Schreyer, Helene Marianne and Gollisch, Tim},
  month    = sep,
  year     = {2022},
  pmid     = {35676096},
  keywords = {Retina, natural stimuli, Priority:0, retina, Visual Perception, receptive field, circuit mechanisms, computational modeling, neural code, review},
  pages    = {171--193}
}

@article{kastner_coordinated_2011,
  title    = {Coordinated dynamic encoding in the retina using opposing forms of plasticity},
  volume   = {14},
  issn     = {1546-1726},
  doi      = {10.1038/nn.2906},
  abstract = {The range of natural inputs encoded by a neuron often exceeds its dynamic range. To overcome this limitation, neural populations divide their inputs among different cell classes, as with rod and cone photoreceptors, and adapt by shifting their dynamic range. We report that the dynamic behavior of retinal ganglion cells in salamanders, mice and rabbits is divided into two opposing forms of short-term plasticity in different cell classes. One population of cells exhibited sensitization-a persistent elevated sensitivity following a strong stimulus. This newly observed dynamic behavior compensates for the information loss caused by the known process of adaptation occurring in a separate cell population. The two populations divide the dynamic range of inputs, with sensitizing cells encoding weak signals and adapting cells encoding strong signals. In the two populations, the linear, threshold and adaptive properties are linked to preserve responsiveness when stimulus statistics change, with one population maintaining the ability to respond when the other fails.},
  language = {eng},
  number   = {10},
  journal  = {Nature Neuroscience},
  author   = {Kastner, David B. and Baccus, Stephen A.},
  month    = sep,
  year     = {2011},
  pmid     = {21909086},
  pmcid    = {PMC3359137},
  keywords = {Retina, Animals, Models, Neurological, Nonlinear Dynamics, Retinal Ganglion Cells, Visual Fields, Action Potentials, Priority:2, Adaptation, Ocular, Photic Stimulation, Rabbits, Mice, Ambystoma, Information Theory, Neuronal Plasticity, Sensitivity and Specificity},
  pages    = {1317--1322},
  file     = {Version acceptée:/home/baptistel/snap/zotero-snap/common/Zotero/storage/7F9ACCM5/Kastner et Baccus - 2011 - Coordinated dynamic encoding in the retina using o.pdf:application/pdf}
}

@article{kastner_insights_2014,
  title    = {Insights from the retina into the diverse and general computations of adaptation, detection, and prediction},
  volume   = {25},
  issn     = {1873-6882},
  doi      = {10.1016/j.conb.2013.11.012},
  abstract = {The retina performs a diverse set of complex, nonlinear, computations, beyond the simple linear photoreceptor weighting assumed in the classical understanding of ganglion cell receptive fields. Here we attempt to organize these computations and extract rules that correspond to three distinct goals of early sensory systems. First, the retina acts efficiently to transmit information to the higher brain for further processing. We observe that although the retina adapts to a number of complex statistics, many of these may be explained by local adaptation to the mean signal strength at that stage. Second, ganglion cells signal the detection of a diverse set of features. Recent results indicate that feature selectivity arises through the action of specific inhibition, rather than through the convergence of excitation as in classical cortical models. Finally, the retina conveys predictions about the stimulus, a function usually attributed only to the higher brain. We expect that computational and mechanistic rules associated with these classes of functions will be an important guide in the study of other neural circuits.},
  language = {eng},
  journal  = {Current Opinion in Neurobiology},
  author   = {Kastner, David B. and Baccus, Stephen A.},
  month    = apr,
  year     = {2014},
  pmid     = {24709602},
  keywords = {Retina, Animals, Adaptation, Physiological, Priority:1, Humans, Nerve Net},
  pages    = {63--69},
  file     = {Full Text:/home/baptistel/snap/zotero-snap/common/Zotero/storage/SWZDUJVT/Kastner and Baccus - 2014 - Insights from the retina into the diverse and gene.pdf:application/pdf}
}

@article{kastner_spatial_2013,
  title    = {Spatial segregation of adaptation and predictive sensitization in retinal ganglion cells},
  volume   = {79},
  issn     = {1097-4199},
  doi      = {10.1016/j.neuron.2013.06.011},
  abstract = {Sensory systems change their sensitivity based on recent stimuli to adjust their response range to the range of inputs and to predict future sensory input. Here, we report the presence of retinal ganglion cells that have antagonistic plasticity, showing central adaptation and peripheral sensitization. Ganglion cell responses were captured by a spatiotemporal model with independently adapting excitatory and inhibitory subunits, and sensitization requires GABAergic inhibition. Using a simple theory of signal detection, we show that the sensitizing surround conforms to an optimal inference model that continually updates the prior signal probability. This indicates that small receptive field regions have dual functionality--to adapt to the local range of signals but sensitize based upon the probability of the presence of that signal. Within this framework, we show that sensitization predicts the location of a nearby object, revealing prediction as a functional role for adapting inhibition in the nervous system.},
  language = {eng},
  number   = {3},
  journal  = {Neuron},
  author   = {Kastner, David B. and Baccus, Stephen A.},
  month    = aug,
  year     = {2013},
  pmid     = {23932000},
  pmcid    = {PMC4046856},
  keywords = {Retina, Animals, Retinal Ganglion Cells, Visual Fields, read, Action Potentials, Neural Inhibition, Adaptation, Physiological, Contrast Sensitivity, Models, Biological, Photic Stimulation, Visual Pathways, adaptation, Ambystoma, Aminobutyrates, Dose-Response Relationship, Drug, Excitatory Amino Acid Agonists, GABA Antagonists, Glycine Agents, Larva, Picrotoxin, Predictive Value of Tests, Signal Detection, Psychological, Strychnine},
  pages    = {541--554},
  file     = {Texte intégral:/home/baptistel/snap/zotero-snap/common/Zotero/storage/Z57RR2UF/Kastner et Baccus - 2013 - Spatial segregation of adaptation and predictive s.pdf:application/pdf}
}

@article{kerschensteiner_feature_2022,
  title    = {Feature {Detection} by {Retinal} {Ganglion} {Cells}},
  volume   = {8},
  issn     = {2374-4650},
  doi      = {10.1146/annurev-vision-100419-112009},
  abstract = {Retinal circuits transform the pixel representation of photoreceptors into the feature representations of ganglion cells, whose axons transmit these representations to the brain. Functional, morphological, and transcriptomic surveys have identified more than 40 retinal ganglion cell (RGC) types in mice. RGCs extract features of varying complexity; some simply signal local differences in brightness (i.e., luminance contrast), whereas others detect specific motion trajectories. To understand the retina, we need to know how retinal circuits give rise to the diverse RGC feature representations. A catalog of the RGC feature set, in turn, is fundamental to understanding visual processing in the brain. Anterograde tracing indicates that RGCs innervate more than 50 areas in the mouse brain. Current maps connecting RGC types to brain areas are rudimentary, as is our understanding of how retinal signals are transformed downstream to guide behavior. In this article, I review the feature selectivities of mouse RGCs, how they arise, and how they are utilized downstream. Not only is knowledge of the behavioral purpose of RGC signals critical for understanding the retinal contributions to vision; it can also guide us to the most relevant areas of visual feature space.},
  language = {eng},
  journal  = {Annual Review of Vision Science},
  author   = {Kerschensteiner, Daniel},
  month    = sep,
  year     = {2022},
  pmid     = {35385673},
  keywords = {Retina, Animals, Retinal Ganglion Cells, Vision, Ocular, read, Mice, Brain, Axons, direction selectivity, looming, luminance contrast, object motion, orientation selectivity, receptive field},
  pages    = {135--169}
}

@article{khani_linear_2021,
  title     = {Linear and nonlinear chromatic integration in the mouse retina},
  volume    = {12},
  copyright = {2021 The Author(s)},
  issn      = {2041-1723},
  url       = {https://www.nature.com/articles/s41467-021-22042-1},
  doi       = {10.1038/s41467-021-22042-1},
  abstract  = {The computations performed by a neural circuit depend on how it integrates its input signals into an output of its own. In the retina, ganglion cells integrate visual information over time, space, and chromatic channels. Unlike the former two, chromatic integration is largely unexplored. Analogous to classical studies of spatial integration, we here study chromatic integration in mouse retina by identifying chromatic stimuli for which activation from the green or UV color channel is maximally balanced by deactivation through the other color channel. This reveals nonlinear chromatic integration in subsets of On, Off, and On–Off ganglion cells. Unlike the latter two, nonlinear On cells display response suppression rather than activation under balanced chromatic stimulation. Furthermore, nonlinear chromatic integration occurs independently of nonlinear spatial integration, depends on contributions from the rod pathway and on surround inhibition, and may provide information about chromatic boundaries, such as the skyline in natural scenes.},
  language  = {en},
  number    = {1},
  urldate   = {2023-06-07},
  journal   = {Nature Communications},
  author    = {Khani, Mohammad Hossein and Gollisch, Tim},
  month     = mar,
  year      = {2021},
  note      = {Number: 1
               Publisher: Nature Publishing Group},
  keywords  = {Priority:1, Visual system, Sensory processing},
  pages     = {1900},
  file      = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/ATB4NU3H/Khani and Gollisch - 2021 - Linear and nonlinear chromatic integration in the .pdf:application/pdf}
}

@article{kim_nonlinear_2020,
  title    = {Nonlinear decoding of natural images from large-scale primate retinal ganglion recordings},
  abstract = {Decoding sensory stimuli from neural activity can provide insight into how the nervous system might interpret the physical environment, and facilitates the development of brain-machine interfaces. Nevertheless, the neural decoding problem remains a significant open challenge. Here, we present an efficient nonlinear decoding approach for inferring natural scene stimuli from the spiking activities of retinal ganglion cells (RGCs). Our approach uses neural networks to improve upon existing decoders in both accuracy and scalability. Trained and validated on real retinal spike data from {\textgreater} 1000 simultaneously recorded macaque RGC units, the decoder demonstrates the necessity of nonlinear computations for accurate decoding of the fine structures of visual stimuli. Specifically, high-pass spatial features of natural images can only be decoded using nonlinear techniques, while low-pass features can be extracted equally well by linear and nonlinear methods. Together, these results advance the state of the art in decoding natural stimuli from large populations of neurons.},
  language = {en},
  author   = {Kim, Young Joon and Brackbill, Nora and Batty, Ella and Lee, JinHyung and Mitelut, Catalin and Tong, William and Chichilnisky, E J and Paninski, Liam},
  year     = {2020},
  keywords = {read, decoding},
  file     = {Kim et al. - 2020 - Nonlinear decoding of natural images from large-sc.pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/HLDP2ZVV/Kim et al. - 2020 - Nonlinear decoding of natural images from large-sc.pdf:application/pdf},
  journal  = {bioRxiv}
}

@article{kim_slow_2003,
  title    = {Slow {Na}+ inactivation and variance adaptation in salamander retinal ganglion cells},
  volume   = {23},
  issn     = {1529-2401},
  doi      = {10.1523/JNEUROSCI.23-04-01506.2003},
  abstract = {The retina adapts to the temporal contrast of the light inputs. One component of contrast adaptation is intrinsic to retinal ganglion cells: temporal contrast affects the variance of the synaptic inputs to ganglion cells, which alters the gain of spike generation. Here we show that slow Na+ inactivation is sufficient to produce the observed variance adaptation. Slow inactivation caused the Na+ current available for spike generation to depend on the past history of activity, both action potentials and subthreshold voltage variations. Recovery from slow inactivation required several hundred milliseconds. Increased current variance caused the threshold for spike generation to increase, presumably because of the decrease in available Na+ current. Simulations indicated that slow Na+ inactivation could account for the observed decrease in excitability. This suggests a simple picture of how ganglion cells contribute to contrast adaptation: (1) increasing contrast causes an increase in input current variance that raises the spike rate, and (2) the increased spike rate reduces the available Na+ current through slow inactivation, which feeds back to reduce excitability. Cells throughout the nervous system face similar problems of accommodating a large range of input signals; furthermore, the Na+ currents of many cells exhibit slow inactivation. Thus, adaptation mediated by feedback modulation of the Na+ current through slow inactivation could serve as a general mechanism to control excitability in spiking neurons.},
  language = {eng},
  number   = {4},
  journal  = {The Journal of Neuroscience: The Official Journal of the Society for Neuroscience},
  author   = {Kim, Kerry J. and Rieke, Fred},
  month    = feb,
  year     = {2003},
  pmid     = {12598639},
  pmcid    = {PMC6742238},
  keywords = {Animals, Retinal Ganglion Cells, Urodela, Adaptation, Action Potentials, Adaptation, Physiological, Cells, Cultured, Computer Simulation, Electric Conductivity, Kinetics, Patch-Clamp Techniques, Priority:1, Sodium Channels, Synapse},
  pages    = {1506--1516},
  file     = {Texte intégral:/home/baptistel/snap/zotero-snap/common/Zotero/storage/KCYCBCH6/Kim et Rieke - 2003 - Slow Na+ inactivation and variance adaptation in s.pdf:application/pdf}
}

@incollection{kolb_architecture_1995,
  address   = {Salt Lake City (UT)},
  title     = {The {Architecture} of the {Human} {Fovea}},
  copyright = {Copyright: © 2023 Webvision .},
  url       = {http://www.ncbi.nlm.nih.gov/books/NBK554706/},
  abstract  = {We summarize the development, structure, different neural types and neural circuitry in the human fovea. The foveal pit is devoid of rod photoreceptors and of secondary and tertiary neurons, allowing light to directly stimulate cones and give us maximal visual acuity. The circuitry underlying the transmission to the brain occurs at the rim of the fovea. The predominant circuitry is concerned with the ‘private’ cone to midget bipolar cell and midget ganglion cell pathways. Every cone drives two midget bipolar cells and two midget ganglion cells so that the message from a single cone is provided to the brain as a contrast between lighter signals (ON pathways) or darker signals (OFF pathways). The sharpening of this contrast message is provided by horizontal-cell feedback circuits and, in some pathways by amacrine circuitry. These midget pathways carry a concentric color and spatially opponent message from red and green cones. Blue cones are sparse, even largely missing in the foveal center while occurring at somewhat higher density than elsewhere in the cone mosaic of the foveal slope. Signals from blue cones have different pathways to ganglion cells. The best understood is through an ON-type blue-cone-selecting bipolar cell to a non-midget, small bistratified ganglion cell. An OFF-center blue midget bipolar is known to be present in the fovea and connects to a blue OFF midget ganglion cell. Another OFF blue message is sent to a giant melanopsin ganglion cell that is present in the foveal rim area, but the circuitry driving that is less certain and possibly involves an intermediate amacrine cell. The H2 horizontal cells are thought to be feedback neurons primarily of the blue cone system. Amacrine cells of the fovea are mostly small-field and glycinergic. The larger field GABAergic amacrines are present but more typically surround the fovea in a ring of processes, with little or no penetration into the foveal center. Thus, the small field glycinergic amacrines are important in some sort of interplay with the midget bipolar–midget ganglion cell channels. We have anatomical descriptions of their synaptology but only a few have been recorded from physiologically. Both OFF pathway and ON pathway amacrines are present in the fovea.},
  language  = {eng},
  urldate   = {2023-06-14},
  booktitle = {Webvision: {The} {Organization} of the {Retina} and {Visual} {System}},
  publisher = {University of Utah Health Sciences Center},
  author    = {Kolb, Helga and Nelson, Ralph F. and Ahnelt, Peter K. and Ortuño-Lizarán, Isabel and Cuenca, Nicolas},
  editor    = {Kolb, Helga and Fernandez, Eduardo and Nelson, Ralph},
  year      = {1995},
  pmid      = {32129967},
  file      = {Printable HTML:/home/baptistel/snap/zotero-snap/common/Zotero/storage/FB8IQ398/NBK554706.html:text/html}
}

@article{kriegeskorte_representational_2008,
  title    = {Representational similarity analysis - connecting the branches of systems neuroscience},
  volume   = {2},
  issn     = {1662-5137},
  url      = {https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008},
  abstract = {A fundamental challenge for systems neuroscience is to quantitatively relate its three major branches of research: brain-activity measurement, behavioral measurement, and computational modeling. Using measured brain-activity patterns to evaluate computational network models is complicated by the need to define the correspondency between the units of the model and the channels of the brain-activity data, e.g., single-cell recordings or voxels from functional magnetic resonance imaging (fMRI). Similar correspondency problems complicate relating activity patterns between different modalities of brain-activity measurement (e.g., fMRI and invasive or scalp electrophysiology), and between subjects and species. In order to bridge these divides, we suggest abstracting from the activity patterns themselves and computing representational dissimilarity matrices (RDMs), which characterize the information carried by a given representation in a brain or model. Building on a rich psychological and mathematical literature on similarity analysis, we propose a new experimental and data-analytical framework called representational similarity analysis (RSA), in which multi-channel measures of neural activity are quantitatively related to each other and to computational theory and behavior by comparing RDMs. We demonstrate RSA by relating representations of visual objects as measured with fMRI in early visual cortex and the fusiform face area to computational models spanning a wide range of complexities. The RDMs are simultaneously related via second-level application of multidimensional scaling and tested using randomization and bootstrap techniques. We discuss the broad potential of RSA, including novel approaches to experimental design, and argue that these ideas, which have deep roots in psychology and neuroscience, will allow the integrated quantitative analysis of data from all three branches, thus contributing to a more unified systems neuroscience.},
  urldate  = {2023-08-29},
  journal  = {Frontiers in Systems Neuroscience},
  author   = {Kriegeskorte, Nikolaus and Mur, Marieke and Bandettini, Peter},
  year     = {2008},
  keywords = {Priority:1, model},
  file     = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/6452J3QX/Kriegeskorte et al. - 2008 - Representational similarity analysis - connecting .pdf:application/pdf}
}

@misc{lappalainen_connectome-constrained_2023,
  title     = {Connectome-constrained deep mechanistic networks predict neural responses across the fly visual system at single-neuron resolution},
  copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  url       = {https://www.biorxiv.org/content/10.1101/2023.03.11.532232v1},
  doi       = {10.1101/2023.03.11.532232},
  abstract  = {We can now measure the connectivity of every neuron in a neural circuit, but we are still blind to other biological details, including the dynamical characteristics of each neuron. The degree to which connectivity measurements alone can inform understanding of neural computation is an open question. Here we show that with only measurements of the connectivity of a biological neural network, we can predict the neural activity underlying neural computation. We constructed a model neural network with the experimentally determined connectivity for 64 cell types in the motion pathways of the fruit fly optic lobe but with unknown parameters for the single neuron and single synapse properties. We then optimized the values of these unknown parameters using techniques from deep learning, to allow the model network to detect visual motion. Our mechanistic model makes detailed experimentally testable predictions for each neuron in the connectome. We found that model predictions agreed with experimental measurements of neural activity across 24 studies. Our work demonstrates a strategy for generating detailed hypotheses about the mechanisms of neural circuit function from connectivity measurements. We show that this strategy is more likely to be successful when neurons are sparsely connected—a universally observed feature of biological neural networks across species and brain regions.},
  language  = {en},
  urldate   = {2023-06-28},
  publisher = {bioRxiv},
  author    = {Lappalainen, Janne K. and Tschopp, Fabian D. and Prakhya, Sridhama and McGill, Mason and Gruntman, Eyal and Macke, Jakob H. and Turaga, Srinivas C.},
  month     = mar,
  year      = {2023},
  note      = {Pages: 2023.03.11.532232
               Section: New Results},
  keywords  = {Coding, Cortex, read, CNN, DL, GP, Brain, Natural Videos},
  file      = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/QWBVKSAZ/Lappalainen et al. - 2023 - Connectome-constrained deep mechanistic networks p.pdf:application/pdf}
}

@misc{lindsey_unified_2019,
  title     = {A {Unified} {Theory} of {Early} {Visual} {Representations} from {Retina} to {Cortex} through {Anatomically} {Constrained} {Deep} {CNNs}},
  url       = {http://arxiv.org/abs/1901.00945},
  doi       = {10.48550/arXiv.1901.00945},
  abstract  = {The visual system is hierarchically organized to process visual information in successive stages. Neural representations vary drastically across the first stages of visual processing: at the output of the retina, ganglion cell receptive fields (RFs) exhibit a clear antagonistic center-surround structure, whereas in the primary visual cortex, typical RFs are sharply tuned to a precise orientation. There is currently no unified theory explaining these differences in representations across layers. Here, using a deep convolutional neural network trained on image recognition as a model of the visual system, we show that such differences in representation can emerge as a direct consequence of different neural resource constraints on the retinal and cortical networks, and we find a single model from which both geometries spontaneously emerge at the appropriate stages of visual processing. The key constraint is a reduced number of neurons at the retinal output, consistent with the anatomy of the optic nerve as a stringent bottleneck. Second, we find that, for simple cortical networks, visual representations at the retinal output emerge as nonlinear and lossy feature detectors, whereas they emerge as linear and faithful encoders of the visual scene for more complex cortices. This result predicts that the retinas of small vertebrates should perform sophisticated nonlinear computations, extracting features directly relevant to behavior, whereas retinas of large animals such as primates should mostly encode the visual scene linearly and respond to a much broader range of stimuli. These predictions could reconcile the two seemingly incompatible views of the retina as either performing feature extraction or efficient coding of natural scenes, by suggesting that all vertebrates lie on a spectrum between these two objectives, depending on the degree of neural resources allocated to their visual system.},
  urldate   = {2023-08-29},
  publisher = {arXiv},
  author    = {Lindsey, Jack and Ocko, Samuel A. and Ganguli, Surya and Deny, Stephane},
  month     = jan,
  year      = {2019},
  note      = {arXiv:1901.00945 [cs, q-bio]},
  keywords  = {Priority:0, DL, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, mechanistic modelling},
  file      = {arXiv Fulltext PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/D6J2CARE/Lindsey et al. - 2019 - A Unified Theory of Early Visual Representations f.pdf:application/pdf;arXiv.org Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/7Y53JD4M/1901.html:text/html}
}

@article{liu_posterior_2023,
  title      = {Posterior {Estimation} {Using} {Deep} {Learning}: {A} {Simulation} {Study} of {Compartmental} {Modeling} in {Dynamic} {PET}},
  issn       = {2331-8422},
  shorttitle = {Posterior {Estimation} {Using} {Deep} {Learning}},
  abstract   = {BACKGROUND: In medical imaging, images are usually treated as deterministic, while their uncertainties are largely underexplored.
                PURPOSE: This work aims at using deep learning to efficiently estimate posterior distributions of imaging parameters, which in turn can be used to derive the most probable parameters as well as their uncertainties.
                METHODS: Our deep learning-based approaches are based on a variational Bayesian inference framework, which is implemented using two different deep neural networks based on conditional variational auto-encoder (CVAE), CVAE-dual-encoder and CVAE-dual-decoder. The conventional CVAE framework, i.e., CVAE-vanilla, can be regarded as a simplified case of these two neural networks. We applied these approaches to a simulation study of dynamic brain PET imaging using a reference region-based kinetic model.
                RESULTS: In the simulation study, we estimated posterior distributions of PET kinetic parameters given a measurement of time-activity curve. Our proposed CVAE-dual-encoder and CVAE-dual-decoder yield results that are in good agreement with the asymptotically unbiased posterior distributions sampled by Markov Chain Monte Carlo (MCMC). The CVAE-vanilla can also be used for estimating posterior distributions, although it has an inferior performance to both CVAE-dual-encoder and CVAE-dual-decoder.
                CONCLUSIONS: We have evaluated the performance of our deep learning approaches for estimating posterior distributions in dynamic brain PET. Our deep learning approaches yield posterior distributions, which are in good agreement with unbiased distributions estimated by MCMC. All these neural networks have different characteristics and can be chosen by the user for specific applications. The proposed methods are general and can be adapted to other problems.},
  language   = {eng},
  journal    = {ArXiv},
  author     = {Liu, Xiaofeng and Marin, Thibault and Amal, Tiss and Woo, Jonghye and Fakhri, Georges El and Ouyang, Jinsong},
  month      = mar,
  year       = {2023},
  pmid       = {36994161},
  pmcid      = {PMC10055492},
  pages      = {arXiv:2303.10057v1}
}

@article{lombrozo_explanation_2014,
  title      = {Explanation and inference: mechanistic and functional explanations guide property generalization},
  volume     = {8},
  issn       = {1662-5161},
  shorttitle = {Explanation and inference},
  url        = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4160962/},
  doi        = {10.3389/fnhum.2014.00700},
  abstract   = {The ability to generalize from the known to the unknown is central to learning and inference. Two experiments explore the relationship between how a property is explained and how that property is generalized to novel species and artifacts. The experiments contrast the consequences of explaining a property mechanistically, by appeal to parts and processes, with the consequences of explaining the property functionally, by appeal to functions and goals. The findings suggest that properties that are explained functionally are more likely to be generalized on the basis of shared functions, with a weaker relationship between mechanistic explanations and generalization on the basis of shared parts and processes. The influence of explanation type on generalization holds even though all participants are provided with the same mechanistic and functional information, and whether an explanation type is freely generated (Experiment 1), experimentally provided (Experiment 2), or experimentally induced (Experiment 2). The experiments also demonstrate that explanations and generalizations of a particular type (mechanistic or functional) can be experimentally induced by providing sample explanations of that type, with a comparable effect when the sample explanations come from the same domain or from a different domains. These results suggest that explanations serve as a guide to generalization, and contribute to a growing body of work supporting the value of distinguishing mechanistic and functional explanations.},
  urldate    = {2023-07-11},
  journal    = {Frontiers in Human Neuroscience},
  author     = {Lombrozo, Tania and Gwynne, Nicholas Z.},
  month      = sep,
  year       = {2014},
  pmid       = {25309384},
  pmcid      = {PMC4160962},
  pages      = {700},
  file       = {PubMed Central Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/ICHTCN63/Lombrozo and Gwynne - 2014 - Explanation and inference mechanistic and functio.pdf:application/pdf}
}

@incollection{lombrozo_mechanistic_2019,
  title     = {Mechanistic versus {Functional} {Understanding}},
  isbn      = {978-0-19-086097-4},
  url       = {https://doi.org/10.1093/oso/9780190860974.003.0011},
  abstract  = {Many natural and artificial entities can be predicted and explained both mechanistically, in term of parts and proximate causal processes, as well as functionally, in terms of functions and goals. Do these distinct “stances” or “modes of construal” support fundamentally different kinds of understanding? Based on recent work in epistemology and philosophy of science, as well as empirical evidence from cognitive and developmental psychology, this chapter argues for the “weak differentiation thesis”: the claim that mechanistic and functional understanding are distinct in that they involve importantly different objects. The chapter also considers more tentative arguments for the “strong differentiation thesis”: the claim that mechanistic and functional understanding involve different epistemic relationships between mind and world.},
  urldate   = {2023-07-11},
  booktitle = {Varieties of {Understanding}: {New} {Perspectives} from {Philosophy}, {Psychology}, and {Theology}},
  publisher = {Oxford University Press},
  author    = {Lombrozo, Tania and Wilkenfeld, Daniel},
  editor    = {Grimm, Stephen R.},
  month     = oct,
  year      = {2019},
  doi       = {10.1093/oso/9780190860974.003.0011},
  keywords  = {priority:2},
  pages     = {0},
  file      = {Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/Y835NJ9U/299833282.html:text/html}
}

@misc{luccioni_estimating_2022,
  title     = {Estimating the {Carbon} {Footprint} of {BLOOM}, a {176B} {Parameter} {Language} {Model}},
  url       = {http://arxiv.org/abs/2211.02001},
  abstract  = {Progress in machine learning (ML) comes with a cost to the environment, given that training ML models requires signiﬁcant computational resources, energy and materials. In the present article, we aim to quantify the carbon footprint of BLOOM, a 176-billion parameter language model, across its life cycle. We estimate that BLOOM’s ﬁnal training emitted approximately 24.7 tonnes of CO2eq if we consider only the dynamic power consumption, and 50.5 tonnes if we account for all processes ranging from equipment manufacturing to energy-based operational consumption. We also study the energy requirements and carbon emissions of its deployment for inference via an API endpoint receiving user queries in real-time. We conclude with a discussion regarding the difﬁculty of precisely estimating the carbon footprint of ML models and future research directions that can contribute towards improving carbon emissions reporting.},
  language  = {en},
  urldate   = {2023-05-15},
  publisher = {arXiv},
  author    = {Luccioni, Alexandra Sasha and Viguier, Sylvain and Ligozat, Anne-Laure},
  month     = nov,
  year      = {2022},
  note      = {arXiv:2211.02001 [cs]},
  keywords  = {Ecology, Computer Science - Machine Learning},
  file      = {Luccioni et al. - 2022 - Estimating the Carbon Footprint of BLOOM, a 176B P.pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/LWSGSR5A/Luccioni et al. - 2022 - Estimating the Carbon Footprint of BLOOM, a 176B P.pdf:application/pdf}
}

@techreport{maheswaranathan_dynamic_2018,
  type        = {preprint},
  title       = {The dynamic neural code of the retina for natural scenes},
  url         = {http://biorxiv.org/lookup/doi/10.1101/340943},
  abstract    = {Understanding how the visual system encodes natural scenes is a fundamental goal of sensory neuroscience. We show here that a three-layer network model predicts the retinal response to natural scenes with an accuracy nearing the fundamental limits of predictability. The model’s internal structure is interpretable, in that model units are highly correlated with interneurons recorded separately and not used to fit the model. We further show the ethological relevance to natural visual processing of a diverse set of phenomena of complex motion encoding, adaptation and predictive coding. Our analysis uncovers a fast timescale of visual processing that is inaccessible directly from experimental data, showing unexpectedly that ganglion cells signal in distinct modes by rapidly ({\textless} 0.1 s) switching their selectivity for direction of motion, orientation, location and the sign of intensity. A new approach that decomposes ganglion cell responses into the contribution of interneurons reveals how the latent effects of parallel retinal circuits generate the response to any possible stimulus. These results reveal extremely flexible and rapid dynamics of the retinal code for natural visual stimuli, explaining the need for a large set of interneuron pathways to generate the dynamic neural code for natural scenes.},
  language    = {en},
  urldate     = {2023-07-13},
  institution = {Neuroscience},
  author      = {Maheswaranathan, Niru and McIntosh, Lane T. and Tanaka, Hidenori and Grant, Satchel and Kastner, David B. and Melander, Josh B. and Nayebi, Aran and Brezovec, Luke and Wang, Julia and Ganguli, Surya and Baccus, Stephen A.},
  month       = jun,
  year        = {2018},
  doi         = {10.1101/340943}
}

@article{maheswaranathan_interpreting_2023,
  title      = {Interpreting the retinal neural code for natural scenes: {From} computations to neurons},
  issn       = {08966273},
  shorttitle = {Interpreting the retinal neural code for natural scenes},
  url        = {https://linkinghub.elsevier.com/retrieve/pii/S0896627323004671},
  doi        = {10.1016/j.neuron.2023.06.007},
  abstract   = {Understanding the circuit mechanisms of the visual code for natural scenes is a central goal of sensory neuroscience. We show that a three-layer network model predicts retinal natural scene responses with an accuracy nearing experimental limits. The model’s internal structure is interpretable, as interneurons recorded separately and not modeled directly are highly correlated with model interneurons. Models ﬁtted only to natural scenes reproduce a diverse set of phenomena related to motion encoding, adaptation, and predictive coding, establishing their ethological relevance to natural visual computation. A new approach decomposes the computations of model ganglion cells into the contributions of model interneurons, allowing automatic generation of new hypotheses for how interneurons with different spatiotemporal responses are combined to generate retinal computations, including predictive phenomena currently lacking an explanation. Our results demonstrate a uniﬁed and general approach to study the circuit mechanisms of ethological retinal computations under natural visual scenes.},
  language   = {en},
  urldate    = {2023-07-18},
  journal    = {Neuron},
  author     = {Maheswaranathan, Niru and McIntosh, Lane T. and Tanaka, Hidenori and Grant, Satchel and Kastner, David B. and Melander, Joshua B. and Nayebi, Aran and Brezovec, Luke E. and Wang, Julia H. and Ganguli, Surya and Baccus, Stephen A.},
  month      = jul,
  year       = {2023},
  pages      = {S0896627323004671},
  file       = {Interpreting.pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/JMLZDX36/Interpreting.pdf:application/pdf;Maheswaranathan et al. - 2023 - Interpreting the retinal neural code for natural s.pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/WZSEDA8D/Maheswaranathan et al. - 2023 - Interpreting the retinal neural code for natural s.pdf:application/pdf}
}

@article{malach_topography_2002,
  title    = {The topography of high-order human object areas},
  volume   = {6},
  issn     = {1879-307X},
  doi      = {10.1016/s1364-6613(02)01870-3},
  abstract = {Cortical topography is one of the most fundamental organizing principles of cortical areas. One such topography - eccentricity mapping - is present even in high-order, ventral stream visual areas. Within these areas, different object categories have specific eccentricity biases. In particular, faces, letters and words appear to be associated with central visual-field bias, whereas buildings are associated with a peripheral one. We propose that resolution needs are an important factor in organizing object representations: objects whose recognition depends on analysis of fine detail will be associated with central-biased representations, whereas objects whose recognition entails large-scale integration will be more peripherally biased.},
  language = {eng},
  number   = {4},
  journal  = {Trends in Cognitive Sciences},
  author   = {Malach, Rafael and Levy, Ifat and Hasson, Uri},
  month    = apr,
  year     = {2002},
  pmid     = {11912041},
  keywords = {Priority:2, Cortical topography},
  pages    = {176--184}
}

@article{marder_neuromodulation_2012,
  title      = {Neuromodulation of {Neuronal} {Circuits}: {Back} to the {Future}},
  volume     = {76},
  issn       = {0896-6273},
  shorttitle = {Neuromodulation of {Neuronal} {Circuits}},
  url        = {https://www.sciencedirect.com/science/article/pii/S0896627312008173},
  doi        = {10.1016/j.neuron.2012.09.010},
  abstract   = {All nervous systems are subject to neuromodulation. Neuromodulators can be delivered as local hormones, as cotransmitters in projection neurons, and through the general circulation. Because neuromodulators can transform the intrinsic firing properties of circuit neurons and alter effective synaptic strength, neuromodulatory substances reconfigure neuronal circuits, often massively altering their output. Thus, the anatomical connectome provides a minimal structure and the neuromodulatory environment constructs and specifies the functional circuits that give rise to behavior.},
  language   = {en},
  number     = {1},
  urldate    = {2023-06-28},
  journal    = {Neuron},
  author     = {Marder, Eve},
  month      = oct,
  year       = {2012},
  pages      = {1--11},
  file       = {ScienceDirect Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/4MXR9NH2/Marder - 2012 - Neuromodulation of Neuronal Circuits Back to the .pdf:application/pdf;ScienceDirect Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/KCA5XP5A/S0896627312008173.html:text/html}
}

@article{martin_brief_1994,
  title    = {A {Brief} {History} of the “{Feature} {Detector}”},
  volume   = {4},
  doi      = {10.1093/cercor/4.1.1},
  abstract = {The feature detector hypothesis, and its subsequent development into the doctrine that single neurons code for perceptually
              significant events, has been the leitmotiv of most work on sensory systems. In the face of strong competition from the alternative theories of neural networks and oscillating
              ensembles of neurons, the single neuron doctrine retains its grip on the imagination of those working on the neural mechanisms
              of perception.},
  journal  = {Cerebral cortex (New York, N.Y. : 1991)},
  author   = {Martin, Kevan},
  month    = jan,
  year     = {1994},
  pages    = {1--7},
  file     = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/JS8592AL/Martin - 1994 - A Brief History of the “Feature Detector”.pdf:application/pdf}
}

@article{mcintosh_deep_2017,
  title    = {Deep {Learning} {Models} of the {Retinal} {Response} to {Natural} {Scenes}},
  abstract = {A central challenge in sensory neuroscience is to understand neural computations and circuit mechanisms that underlie the encoding of ethologically relevant, natural stimuli. In multilayered neural circuits, nonlinear processes such as synaptic transmission and spiking dynamics present a significant obstacle to the creation of accurate computational models of responses to natural stimuli. Here we demonstrate that deep convolutional neural networks (CNNs) capture retinal responses to natural scenes nearly to within the variability of a cell’s response, and are markedly more accurate than linear-nonlinear (LN) models and Generalized Linear Models (GLMs). Moreover, we find two additional surprising properties of CNNs: they are less susceptible to overfitting than their LN counterparts when trained on small amounts of data, and generalize better when tested on stimuli drawn from a different distribution (e.g. between natural scenes and white noise). An examination of the learned CNNs reveals several properties. First, a richer set of feature maps is necessary for predicting the responses to natural scenes compared to white noise. Second, temporally precise responses to slowly varying inputs originate from feedforward inhibition, similar to known retinal mechanisms. Third, the injection of latent noise sources in intermediate layers enables our model to capture the sub-Poisson spiking variability observed in retinal ganglion cells. Fourth, augmenting our CNNs with recurrent lateral connections enables them to capture contrast adaptation as an emergent property of accurately describing retinal responses to natural scenes. These methods can be readily generalized to other sensory modalities and stimulus ensembles. Overall, this work demonstrates that CNNs not only accurately capture sensory circuit responses to natural scenes, but also can yield information about the circuit’s internal structure and function.},
  language = {en},
  author   = {McIntosh, Lane T and Maheswaranathan, Niru and Nayebi, Aran and Ganguli, Surya and Baccus, Stephen A},
  year     = {2017},
  keywords = {Coding, DL, Read},
  file     = {McIntosh et al. - 2017 - Deep Learning Models of the Retinal Response to Na.pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/N7NTYB6F/McIntosh et al. - 2017 - Deep Learning Models of the Retinal Response to Na.pdf:application/pdf;NIHMS870072-supplement-Supplemental_material.pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/I773ZSB6/NIHMS870072-supplement-Supplemental_material.pdf:application/pdf}
}

@inproceedings{montufar_number_2014,
  title     = {On the {Number} of {Linear} {Regions} of {Deep} {Neural} {Networks}},
  volume    = {27},
  url       = {https://papers.nips.cc/paper_files/paper/2014/hash/109d2dd3608f669ca17920c511c2a41e-Abstract.html},
  abstract  = {We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.},
  urldate   = {2023-06-28},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
  year      = {2014},
  file      = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/YK734GK9/Montufar et al. - 2014 - On the Number of Linear Regions of Deep Neural Net.pdf:application/pdf}
}

@article{nikolaev_synaptic_2013,
  title    = {Synaptic mechanisms of adaptation and sensitization in the retina},
  volume   = {16},
  issn     = {1546-1726},
  doi      = {10.1038/nn.3408},
  abstract = {Sensory systems continually adjust the way stimuli are processed. What are the circuit mechanisms underlying this plasticity? We investigated how synapses in the retina of zebrafish adjust to changes in the temporal contrast of a visual stimulus by imaging activity in vivo. Following an increase in contrast, bipolar cell synapses with strong initial responses depressed, whereas synapses with weak initial responses facilitated. Depression and facilitation predominated in different strata of the inner retina, where bipolar cell output was anticorrelated with the activity of amacrine cell synapses providing inhibitory feedback. Pharmacological block of GABAergic feedback converted facilitating bipolar cell synapses into depressing ones. These results indicate that depression intrinsic to bipolar cell synapses causes adaptation of the ganglion cell response to contrast, whereas depression in amacrine cell synapses causes sensitization. Distinct microcircuits segregating to different layers of the retina can cause simultaneous increases or decreases in the gain of neural responses.},
  language = {eng},
  number   = {7},
  journal  = {Nature Neuroscience},
  author   = {Nikolaev, Anton and Leung, Kin-Mei and Odermatt, Benjamin and Lagnado, Leon},
  month    = jul,
  year     = {2013},
  pmid     = {23685718},
  pmcid    = {PMC3924174},
  keywords = {Retina, Light, Neurons, Animals, Feedback, Physiological, read, Neural Inhibition, Synapses, Synaptic Transmission, Synapse, Adaptation, Ocular, Photic Stimulation, Visual Pathways, adaptation, Picrotoxin, Aminobenzoates, Anesthetics, Animals, Genetically Modified, Calcium, Central Nervous System Stimulants, Parvalbumins, Phosphopyruvate Hydratase, Presynaptic Terminals, Synaptic Vesicles, Zebrafish, Zebrafish Proteins},
  pages    = {934--941},
  file     = {Accepted Version:/home/baptistel/snap/zotero-snap/common/Zotero/storage/WCVAYZAB/Nikolaev et al. - 2013 - Synaptic mechanisms of adaptation and sensitizatio.pdf:application/pdf}
}

@misc{noauthor_chromatix_2023,
  title      = {Chromatix : {Differentiable} wave optics using {JAX}!},
  copyright  = {MIT},
  shorttitle = {Chromatix},
  url        = {https://github.com/chromatix-team/chromatix},
  abstract   = {Differentiable wave optics using JAX! Documentation can be found at https://chromatix.readthedocs.io},
  urldate    = {2023-06-27},
  publisher  = {Chromatix Team},
  month      = jun,
  year       = {2023},
  note       = {original-date: 2022-07-07T20:48:09Z}
}

@misc{noauthor_compartmental_2023,
  title     = {Compartmental neuron models},
  copyright = {Creative Commons Attribution-ShareAlike License},
  url       = {https://en.wikipedia.org/w/index.php?title=Compartmental_neuron_models&oldid=1153535770},
  abstract  = {Compartmental modelling of dendrites deals with multi-compartment modelling of the dendrites, to make the understanding of the electrical behavior of complex dendrites easier. Basically, compartmental modelling of dendrites is a very helpful tool to develop new biological neuron models. Dendrites are very important because they occupy the most membrane area in many of the neurons and give the neuron an ability to connect to thousands of other cells. Originally the dendrites were thought to have constant conductance and current but now it has been understood that they may have active Voltage-gated ion channels, which influences the firing properties of the neuron and also the response of neuron to synaptic inputs. Many mathematical models have been developed to understand the electric behavior of the dendrites. Dendrites tend to be very branchy and complex, so the compartmental approach to understand the electrical behavior of the dendrites makes it very useful.},
  language  = {en},
  urldate   = {2023-05-10},
  journal   = {Wikipedia},
  month     = may,
  year      = {2023},
  note      = {Page Version ID: 1153535770}
}

@misc{noauthor_data-driven_nodate,
  title    = {Data-driven emergence of convolutional structure in neural networks {\textbar} {PNAS}},
  url      = {https://www.pnas.org/doi/10.1073/pnas.2201854119},
  urldate  = {2023-05-02},
  keywords = {read, CNN, DL},
  file     = {Data-driven emergence of convolutional structure in neural networks | PNAS:/home/baptistel/snap/zotero-snap/common/Zotero/storage/TJ9ADWI6/pnas.html:text/html;Full Text:/home/baptistel/snap/zotero-snap/common/Zotero/storage/TNDK48KE/Data-driven emergence of convolutional structure i.pdf:application/pdf}
}

@misc{noauthor_dendritic_nodate,
  title   = {Dendritic {Integration} and {Its} {Role} in {Computing} {Image} {Velocity} {\textbar} {Science}},
  url     = {https://www.science.org/doi/abs/10.1126/science.281.5384.1848},
  urldate = {2023-06-19},
  file    = {Dendritic Integration and Its Role in Computing Image Velocity | Science:/home/baptistel/snap/zotero-snap/common/Zotero/storage/3TF3NKZ9/science.281.5384.html:text/html}
}

@misc{noauthor_discovering_nodate,
  title    = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems {\textbar} {PNAS}},
  url      = {https://www.pnas.org/doi/abs/10.1073/pnas.1517384113},
  urldate  = {2023-05-24},
  keywords = {Priority:3}
}

@misc{noauthor_dynamic_nodate,
  title    = {The dynamic neural code of the retina for natural scenes {\textbar} {bioRxiv}},
  url      = {https://www.biorxiv.org/content/10.1101/340943v5},
  urldate  = {2023-05-15},
  keywords = {Adaptation, read, Natural Images},
  file     = {Maheswaranathan et al. - 2018 - The dynamic neural code of the retina for natural .pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/Q9B24Z3N/Maheswaranathan et al. - 2018 - The dynamic neural code of the retina for natural .pdf:application/pdf;The dynamic neural code of the retina for natural scenes | bioRxiv:/home/baptistel/snap/zotero-snap/common/Zotero/storage/H3IRXEA5/340943v5.html:text/html}
}

@misc{noauthor_fast_nodate,
  title    = {Fast {Recurrent} {Processing} via {Ventrolateral} {Prefrontal} {Cortex} {Is} {Needed} by the {Primate} {Ventral} {Stream} for {Robust} {Core} {Visual} {Object} {Recognition} - {ScienceDirect}},
  url      = {https://www.sciencedirect.com/science/article/pii/S0896627320307595},
  urldate  = {2023-05-04},
  keywords = {Adaptation, Cortex, Priority:2},
  file     = {Fast Recurrent Processing via Ventrolateral Prefrontal Cortex Is Needed by the Primate Ventral Stream for Robust Core Visual Object Recognition - ScienceDirect:/home/baptistel/snap/zotero-snap/common/Zotero/storage/QIDM5REB/S0896627320307595.html:text/html;Full Text:/home/baptistel/snap/zotero-snap/common/Zotero/storage/VSU6Q2SI/Fast Recurrent Processing via Ventrolateral Prefro.pdf:application/pdf}
}

@misc{noauthor_gain_nodate,
  title   = {Gain modulation from background synaptic input - {PubMed}},
  url     = {https://pubmed.ncbi.nlm.nih.gov/12194875/},
  urldate = {2023-06-19},
  file    = {Gain modulation from background synaptic input - PubMed:/home/baptistel/snap/zotero-snap/common/Zotero/storage/Q5H7CZIW/12194875.html:text/html}
}

@misc{noauthor_is_nodate,
  title    = {Is the {Backwards} {Human} {Retina} {Evidence} of {Poor} {Design}?},
  url      = {https://www.icr.org/article/backwards-human-retina-evidence-poor-design/},
  abstract = {By Jerry Bergman, Ph.D. and Joseph Calkins, M.D.*
              
              Introduction
              
              The so-called backwards retina is an example of an argument against creationism long ago disproved. Nonetheless, it is one of the most common arguments used by Darwinists to argue that life was not designed. For example, one of the leading American Darwinists, Brown University Professor Kenneth Miller, claimed that a prime example of "poor design" is the fact that light in the human eye has to travel through the neuron lay},
  language = {en},
  urldate  = {2023-07-05},
  file     = {Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/XJMS53H4/backwards-human-retina-evidence-poor-design.html:text/html}
}

@misc{noauthor_large_nodate,
  title    = {‪{Large} scale image segmentation with structured loss based deep learning for connectome reconstruction‬},
  url      = {https://scholar.google.co.uk/citations?view_op=view_citation&hl=en&user=V_NdI3sAAAAJ&citft=1&email_for_op=lorenzibaptiste4%40gmail.com&citation_for_view=V_NdI3sAAAAJ:IWHjjKOFINEC},
  abstract = {‪J Funke, F Tschopp, W Grisaitis, A Sheridan, C Singh, S Saalfeld, SC Turaga‬, ‪IEEE transactions on pattern analysis and machine intelligence, 2018‬ - ‪Cited by 190‬},
  urldate  = {2023-06-28},
  keywords = {priority:1, UNet},
  file     = {‪Large scale image segmentation with structured lo.pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/2RWR5SMJ/‪Large scale image segmentation with structured lo.pdf:application/pdf;Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/79IVBW9T/citations.html:text/html}
}

@misc{noauthor_mapping_nodate,
  title    = {Mapping a {Complete} {Neural} {Population} in the {Retina} {\textbar} {Journal} of {Neuroscience}},
  url      = {https://www.jneurosci.org/content/32/43/14859},
  urldate  = {2023-05-15},
  keywords = {Priority:2},
  file     = {Mapping a Complete Neural Population in the Retina | Journal of Neuroscience:/home/baptistel/snap/zotero-snap/common/Zotero/storage/5S92778I/14859.html:text/html}
}

@misc{noauthor_research_nodate,
  title    = {Research {Summary}},
  url      = {http://meisterlab.caltech.edu/},
  language = {en},
  urldate  = {2023-06-07},
  journal  = {The Meister Laboratory}
}

@misc{noauthor_retinal_nodate,
  title   = {Retinal {Computation} - 1st {Edition}},
  url     = {https://www.elsevier.com/books/retinal-computation/schwartz/978-0-12-819896-4},
  urldate = {2023-02-03},
  file    = {Retinal Computation - 1st Edition:/home/baptistel/snap/zotero-snap/common/Zotero/storage/WNY2DQP6/978-0-12-819896-4.html:text/html}
}

@misc{noauthor_theory_nodate,
  title    = {Theory of early vision},
  url      = {http://meisterlab.caltech.edu/research/theory},
  language = {en},
  urldate  = {2023-06-07},
  journal  = {The Meister Laboratory},
  keywords = {Priority:1},
  file     = {Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/5JDQZYNU/theory.html:text/html}
}

@inproceedings{ocko_emergence_2018,
  title     = {The emergence of multiple retinal cell types through efficient coding of natural movies},
  volume    = {31},
  url       = {https://papers.nips.cc/paper_files/paper/2018/hash/d94fd74dcde1aa553be72c1006578b23-Abstract.html},
  abstract  = {One of the most striking aspects of early visual processing in the retina is the immediate parcellation of visual information into multiple parallel pathways, formed by different retinal ganglion cell types each tiling the entire visual field. Existing theories of efficient coding have been unable to account for the functional advantages of such cell-type diversity in encoding natural scenes. Here we go beyond previous theories to analyze how a simple linear retinal encoding model with different convolutional cell types efficiently encodes naturalistic spatiotemporal movies given a fixed firing rate budget. We find that optimizing the receptive fields and cell densities of two cell types makes them match the properties of the two main cell types in the primate retina, midget and parasol cells, in terms of spatial and temporal sensitivity, cell spacing, and their relative ratio. Moreover, our theory gives a precise account of how the ratio of midget to parasol cells decreases with retinal eccentricity.  Also, we train a nonlinear encoding model with a rectifying nonlinearity to efficiently encode naturalistic movies, and again find emergent receptive fields resembling those of midget and parasol cells that are now further subdivided into ON and OFF types. Thus our work provides a theoretical justification, based on the efficient coding of natural movies, for the existence of the four most dominant cell types in the primate retina that together comprise 70\% of all ganglion cells.},
  urldate   = {2023-08-29},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Ocko, Samuel and Lindsey, Jack and Ganguli, Surya and Deny, Stephane},
  year      = {2018},
  keywords  = {Priority:0},
  file      = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/ERJ33XGH/Ocko et al. - 2018 - The emergence of multiple retinal cell types throu.pdf:application/pdf}
}

@article{olshausen_emergence_1996,
  title     = {Emergence of simple-cell receptive field properties by learning a sparse code for natural images},
  volume    = {381},
  copyright = {1996 Springer Nature Limited},
  issn      = {1476-4687},
  url       = {https://www.nature.com/articles/381607a0},
  doi       = {10.1038/381607a0},
  abstract  = {THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1–4 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7–12. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13–18, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.},
  language  = {en},
  number    = {6583},
  urldate   = {2023-05-22},
  journal   = {Nature},
  author    = {Olshausen, Bruno A. and Field, David J.},
  month     = jun,
  year      = {1996},
  note      = {Number: 6583
               Publisher: Nature Publishing Group},
  keywords  = {Coding, Cortex, Natural Images, Humanities and Social Sciences, multidisciplinary, Science},
  pages     = {607--609}
}

@article{olshausen_learning_2001,
  title    = {Learning {Sparse} {Image} {Codes} using a {Wavelet} {Pyramid} {Architecture}},
  abstract = {We show how a wavelet basis may be adapted to best represent natural images in terms of sparse coefficients. The wavelet basis, which may be either complete or overcomplete, is specified by a small number of spatial functions which are repeated across space and combined in a recursive fashion so as to be self-similar across scale. These functions are adapted to minimize the estimated code length under a model that assumes images are composed of a linear superposition of sparse, independent components. When adapted to natural images, the wavelet bases take on different orientations and they evenly tile the orientation domain, in stark contrast to the standard, non-oriented wavelet bases used in image compression. When the basis set is allowed to be overcomplete, it also yields higher coding efficiency than standard wavelet bases. 1 Introduction The general problem we address here is that of learning efficient codes for representing natural images. Our previous work in this...},
  author   = {Olshausen, Bruno and Sallee, Phil and Lewicki, Michael},
  month    = feb,
  year     = {2001},
  keywords = {Coding, Priority:2},
  file     = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/7QGPVC5D/Olshausen et al. - 2001 - Learning Sparse Image Codes using a Wavelet Pyrami.pdf:application/pdf}
}

@article{olshausen_sparse_1997,
  title      = {Sparse coding with an overcomplete basis set: {A} strategy employed by {V1}?},
  volume     = {37},
  issn       = {0042-6989},
  shorttitle = {Sparse coding with an overcomplete basis set},
  url        = {https://www.sciencedirect.com/science/article/pii/S0042698997001697},
  doi        = {10.1016/S0042-6989(97)00169-7},
  abstract   = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete—i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.},
  language   = {en},
  number     = {23},
  urldate    = {2023-07-12},
  journal    = {Vision Research},
  author     = {Olshausen, Bruno A. and Field, David J.},
  month      = dec,
  year       = {1997},
  keywords   = {Coding, priority:1, coding, Gabor-wavelet, Natural images, V1, population, sparse},
  pages      = {3311--3325},
  file       = {ScienceDirect Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/MYXN6JKS/Olshausen and Field - 1997 - Sparse coding with an overcomplete basis set A st.pdf:application/pdf;ScienceDirect Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/QF6H9JF7/S0042698997001697.html:text/html}
}

@article{ozuysal_linking_2012,
  title    = {Linking the {Computational} {Structure} of {Variance} {Adaptation} to {Biophysical} {Mechanisms}},
  volume   = {73},
  issn     = {0896-6273},
  url      = {https://www.sciencedirect.com/science/article/pii/S0896627312000797},
  doi      = {10.1016/j.neuron.2011.12.029},
  abstract = {In multiple sensory systems, adaptation to the variance of a sensory input changes the sensitivity, kinetics, and average response over timescales ranging from {\textless} 100 ms to tens of seconds. Here, we present a simple, biophysically relevant model of retinal contrast adaptation that accurately captures both the membrane potential response and all adaptive properties. The adaptive component of this model is a first-order kinetic process of the type used to describe ion channel gating and synaptic transmission. From the model, we conclude that all adaptive dynamics can be accounted for by depletion of a signaling mechanism, and that variance adaptation can be explained as adaptation to the mean of a rectified signal. The model parameters show strong similarity to known properties of bipolar cell synaptic vesicle pools. Diverse types of adaptive properties that implement theoretical principles of efficient coding can be generated by a single type of molecule or synapse with just a few microscopic states.},
  language = {en},
  number   = {5},
  urldate  = {2023-06-01},
  journal  = {Neuron},
  author   = {Ozuysal, Yusuf and Baccus, Stephen A.},
  month    = mar,
  year     = {2012},
  keywords = {read, adaptation, Compartment, LN},
  pages    = {1002--1015},
  file     = {ScienceDirect Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/8AZ9KMW9/Ozuysal and Baccus - 2012 - Linking the Computational Structure of Variance Ad.pdf:application/pdf;ScienceDirect Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/XWBVYJSC/S0896627312000797.html:text/html}
}

@article{park_receptive_2011,
  title    = {Receptive {Field} {Inference} with {Localized} {Priors}},
  volume   = {7},
  issn     = {1553-7358},
  url      = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002219},
  doi      = {10.1371/journal.pcbi.1002219},
  abstract = {The linear receptive field describes a mapping from sensory stimuli to a one-dimensional variable governing a neuron's spike response. However, traditional receptive field estimators such as the spike-triggered average converge slowly and often require large amounts of data. Bayesian methods seek to overcome this problem by biasing estimates towards solutions that are more likely a priori, typically those with small, smooth, or sparse coefficients. Here we introduce a novel Bayesian receptive field estimator designed to incorporate locality, a powerful form of prior information about receptive field structure. The key to our approach is a hierarchical receptive field model that flexibly adapts to localized structure in both spacetime and spatiotemporal frequency, using an inference method known as empirical Bayes. We refer to our method as automatic locality determination (ALD), and show that it can accurately recover various types of smooth, sparse, and localized receptive fields. We apply ALD to neural data from retinal ganglion cells and V1 simple cells, and find it achieves error rates several times lower than standard estimators. Thus, estimates of comparable accuracy can be achieved with substantially less data. Finally, we introduce a computationally efficient Markov Chain Monte Carlo (MCMC) algorithm for fully Bayesian inference under the ALD prior, yielding accurate Bayesian confidence intervals for small or noisy datasets.},
  language = {en},
  number   = {10},
  urldate  = {2023-05-05},
  journal  = {PLOS Computational Biology},
  author   = {Park, Mijung and Pillow, Jonathan W.},
  month    = oct,
  year     = {2011},
  note     = {Publisher: Public Library of Science},
  keywords = {Neurons, Priority:2, Covariance, Fourier analysis, Gaussian noise, GP, Markov models, Retinal ganglion cells, Simulation and modeling, White noise},
  pages    = {e1002219},
  file     = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/VL7P89B4/Park and Pillow - 2011 - Receptive Field Inference with Localized Priors.pdf:application/pdf}
}

@article{pulido_quantal_2017,
  title      = {Quantal {Fluctuations} in {Central} {Mammalian} {Synapses}: {Functional} {Role} of {Vesicular} {Docking} {Sites}},
  volume     = {97},
  issn       = {1522-1210},
  shorttitle = {Quantal {Fluctuations} in {Central} {Mammalian} {Synapses}},
  doi        = {10.1152/physrev.00032.2016},
  abstract   = {Quantal fluctuations are an integral part of synaptic signaling. At the frog neuromuscular junction, Bernard Katz proposed that quantal fluctuations originate at "reactive sites" where specific structures of the presynaptic membrane interact with synaptic vesicles. However, the physical nature of reactive sites has remained unclear, both at the frog neuromuscular junction and at central synapses. Many central synapses, called simple synapses, are small structures containing a single presynaptic active zone and a single postsynaptic density of receptors. Several lines of evidence indicate that simple synapses may release several synaptic vesicles in response to a single action potential. However, in some synapses at least, each release event activates a significant fraction of the postsynaptic receptors, giving rise to a sublinear relation between vesicular release and postsynaptic current. Partial receptor saturation as well as synaptic jitter gives to simple synapse signaling the appearance of a binary process. Recent investigations of simple synapses indicate that the number of released vesicles follows binomial statistics, with a maximum reflecting the number of docking sites present in the active zone. These results suggest that at central synapses, vesicular docking sites represent the reactive sites proposed by Katz. The macromolecular architecture and molecular composition of docking sites are presently investigated with novel combinations of techniques. It is proposed that variations in docking site numbers are central in defining intersynaptic variability and that docking site occupancy is a key parameter regulating short-term synaptic plasticity.},
  language   = {eng},
  number     = {4},
  journal    = {Physiological Reviews},
  author     = {Pulido, Camila and Marty, Alain},
  month      = oct,
  year       = {2017},
  pmid       = {28835509},
  keywords   = {Animals, Synaptic Transmission, Priority:2, Humans, Synaptic Vesicles, Neuromuscular Junction, synapse},
  pages      = {1403--1430},
  file       = {Full Text:/home/baptistel/snap/zotero-snap/common/Zotero/storage/S7DZI66F/Pulido and Marty - 2017 - Quantal Fluctuations in Central Mammalian Synapses.pdf:application/pdf}
}

@inproceedings{raison-aubry_computational_2023,
  title    = {Computational modeling of the secondary rod pathway contribution to the retinal output},
  doi      = {10.1109/NER52421.2023.10123863},
  author   = {Raison-Aubry, Laetitia and Naudin, Loïs and Jin, Nange and Ribelayga, Christophe and Buhry, Laure},
  month    = apr,
  year     = {2023},
  keywords = {priority:1},
  pages    = {1--4},
  file     = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/UWSKRA7D/Raison-Aubry et al. - 2023 - Computational modeling of the secondary rod pathwa.pdf:application/pdf}
}

@article{rattay_compartment_2018,
  title    = {Compartment models for the electrical stimulation of retinal bipolar cells},
  volume   = {13},
  issn     = {1932-6203},
  url      = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6296559/},
  doi      = {10.1371/journal.pone.0209123},
  abstract = {Bipolar cells of the retina are among the smallest neurons of the nervous system. For this reason, compared to other neurons, their delay in signaling is minimal. Additionally, the small bipolar cell surface combined with the low membrane conductance causes very little attenuation in the signal from synaptic input to the terminal. The existence of spiking bipolar cells was proven over the last two decades, but until now no complete model including all important ion channel types was published. The present study amends this and analyzes the impact of the number of model compartments on simulation accuracy. Characteristic features like membrane voltages and spike generation were tested and compared for one-, two-, four- and 117-compartment models of a macaque bipolar cell. Although results were independent of the compartment number for low membrane conductances (passive membranes), nonlinear regimes such as spiking required at least a separate axon compartment. At least a four compartment model containing the functionally different segments dendrite, soma, axon and terminal was needed for understanding signaling in spiking bipolar cells. Whereas for intracellular current application models with small numbers of compartments showed quantitatively correct results in many cases, the cell response to extracellular stimulation is sensitive to spatial variation of the electric field and accurate modeling therefore demands for a large number of short compartments even for passive membranes.},
  number   = {12},
  urldate  = {2023-05-10},
  journal  = {PLoS ONE},
  author   = {Rattay, Frank and Bassereh, Hassan and Stiennon, Isabel},
  month    = dec,
  year     = {2018},
  pmid     = {30557410},
  pmcid    = {PMC6296559},
  keywords = {Compartment},
  pages    = {e0209123},
  file     = {PubMed Central Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/9N82MRQ4/Rattay et al. - 2018 - Compartment models for the electrical stimulation .pdf:application/pdf}
}

@article{rieke_challenges_2009,
  title    = {The {Challenges} {Natural} {Images} {Pose} for {Visual} {Adaptation}},
  volume   = {64},
  issn     = {0896-6273},
  url      = {https://www.sciencedirect.com/science/article/pii/S0896627309009441},
  doi      = {10.1016/j.neuron.2009.11.028},
  abstract = {Advances in our understanding of natural image statistics and of gain control within the retinal circuitry are leading to new insights into the classic problem of retinal light adaptation. Here we review what we know about how rapid adaptation occurs during active exploration of the visual scene. Adaptational mechanisms must balance the competing demands of adapting quickly, locally, and reliably, and this balance must be maintained as lighting conditions change. Multiple adaptational mechanisms in different locations within the retina act in concert to accomplish this task, with lighting conditions dictating which mechanisms dominate.},
  language = {en},
  number   = {5},
  urldate  = {2023-06-01},
  journal  = {Neuron},
  author   = {Rieke, Fred and Rudd, Michael E.},
  month    = dec,
  year     = {2009},
  keywords = {Priority:1, Natural Images},
  pages    = {605--616},
  file     = {ScienceDirect Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/FXNA7UWR/Rieke and Rudd - 2009 - The Challenges Natural Images Pose for Visual Adap.pdf:application/pdf;ScienceDirect Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/NMA4V5DR/S0896627309009441.html:text/html}
}

@incollection{rosenblith_possible_2012,
  title     = {Possible {Principles} {Underlying} the {Transformations} of {Sensory} {Messages}},
  isbn      = {978-0-262-51842-0},
  url       = {https://academic.oup.com/mit-press-scholarship-online/book/20714/chapter/180090664},
  language  = {en},
  urldate   = {2023-05-22},
  booktitle = {Sensory {Communication}},
  publisher = {The MIT Press},
  author    = {Barlow, H. B.},
  editor    = {Rosenblith, Walter A.},
  month     = sep,
  year      = {2012},
  doi       = {10.7551/mitpress/9780262518420.003.0013},
  keywords  = {Priority:2},
  pages     = {216--234},
  file      = {Barlow - 2012 - Possible Principles Underlying the Transformations.pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/FGUBWCMD/Barlow - 2012 - Possible Principles Underlying the Transformations.pdf:application/pdf}
}

@article{rust_how_2006,
  title    = {How {MT} cells analyze the motion of visual patterns},
  volume   = {9},
  issn     = {1097-6256},
  doi      = {10.1038/nn1786},
  abstract = {Neurons in area MT (V5) are selective for the direction of visual motion. In addition, many are selective for the motion of complex patterns independent of the orientation of their components, a behavior not seen in earlier visual areas. We show that the responses of MT cells can be captured by a linear-nonlinear model that operates not on the visual stimulus, but on the afferent responses of a population of nonlinear V1 cells. We fit this cascade model to responses of individual MT neurons and show that it robustly predicts the separately measured responses to gratings and plaids. The model captures the full range of pattern motion selectivity found in MT. Cells that signal pattern motion are distinguished by having convergent excitatory input from V1 cells with a wide range of preferred directions, strong motion opponent suppression and a tuned normalization that may reflect suppressive input from the surround of V1 cells.},
  language = {eng},
  number   = {11},
  journal  = {Nature Neuroscience},
  author   = {Rust, Nicole C. and Mante, Valerio and Simoncelli, Eero P. and Movshon, J. Anthony},
  month    = nov,
  year     = {2006},
  pmid     = {17041595},
  keywords = {Animals, Models, Neurological, Nonlinear Dynamics, Priority:2, Photic Stimulation, Algorithms, Linear Models, Macaca fascicularis, Macaca nemestrina, Motion Perception, Neurons, Afferent, Pattern Recognition, Physiological, Visual Cortex},
  pages    = {1421--1431}
}

@article{sable-meyer_sensitivity_2021,
  title      = {Sensitivity to geometric shape regularity in humans and baboons: {A} putative signature of human singularity},
  volume     = {118},
  issn       = {1091-6490},
  shorttitle = {Sensitivity to geometric shape regularity in humans and baboons},
  doi        = {10.1073/pnas.2023123118},
  abstract   = {Among primates, humans are special in their ability to create and manipulate highly elaborate structures of language, mathematics, and music. Here we show that this sensitivity to abstract structure is already present in a much simpler domain: the visual perception of regular geometric shapes such as squares, rectangles, and parallelograms. We asked human subjects to detect an intruder shape among six quadrilaterals. Although the intruder was always defined by an identical amount of displacement of a single vertex, the results revealed a geometric regularity effect: detection was considerably easier when either the base shape or the intruder was a regular figure comprising right angles, parallelism, or symmetry rather than a more irregular shape. This effect was replicated in several tasks and in all human populations tested, including uneducated Himba adults and French kindergartners. Baboons, however, showed no such geometric regularity effect, even after extensive training. Baboon behavior was captured by convolutional neural networks (CNNs), but neither CNNs nor a variational autoencoder captured the human geometric regularity effect. However, a symbolic model, based on exact properties of Euclidean geometry, closely fitted human behavior. Our results indicate that the human propensity for symbolic abstraction permeates even elementary shape perception. They suggest a putative signature of human singularity and provide a challenge for nonsymbolic models of human shape perception.},
  language   = {eng},
  number     = {16},
  journal    = {Proceedings of the National Academy of Sciences of the United States of America},
  author     = {Sablé-Meyer, Mathias and Fagot, Joël and Caparos, Serge and van Kerkoerle, Timo and Amalric, Marie and Dehaene, Stanislas},
  month      = apr,
  year       = {2021},
  pmid       = {33846254},
  pmcid      = {PMC8072260},
  keywords   = {Animals, Vision, Ocular, Priority:2, Humans, Neural Networks, Computer, Adult, Child, Preschool, comparative cognition, developmental psychology, Female, Form Perception, geometry, human singularity, Language, Male, Middle Aged, neural network modeling, Papio, Pattern Recognition, Visual, Species Specificity, Visual Perception},
  pages      = {e2023123118},
  file       = {Full Text:/home/baptistel/snap/zotero-snap/common/Zotero/storage/LN675BPC/Sablé-Meyer et al. - 2021 - Sensitivity to geometric shape regularity in human.pdf:application/pdf}
}

@article{schaeffer_learning_2017,
  title    = {Learning partial differential equations via data discovery and sparse optimization},
  volume   = {473},
  url      = {https://royalsocietypublishing.org/doi/full/10.1098/rspa.2016.0446},
  doi      = {10.1098/rspa.2016.0446},
  abstract = {We investigate the problem of learning an evolution equation directly from some given data. This work develops a learning algorithm to identify the terms in the underlying partial differential equations and to approximate the coefficients of the terms only using data. The algorithm uses sparse optimization in order to perform feature selection and parameter estimation. The features are data driven in the sense that they are constructed using nonlinear algebraic equations on the spatial derivatives of the data. Several numerical experiments show the proposed method's robustness to data noise and size, its ability to capture the true features of the data, and its capability of performing additional analytics. Examples include shock equations, pattern formation, fluid flow and turbulence, and oscillatory convection.},
  number   = {2197},
  urldate  = {2023-05-24},
  journal  = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  author   = {Schaeffer, Hayden},
  month    = jan,
  year     = {2017},
  note     = {Publisher: Royal Society},
  keywords = {Priority:2, feature selection, machine learning, parameter estimation, partial differential equations, sparse optimization},
  pages    = {20160446},
  file     = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/CDPB776S/Schaeffer - 2017 - Learning partial differential equations via data d.pdf:application/pdf}
}

@article{schreyer_nonlinear_2021,
  title    = {Nonlinear spatial integration in retinal bipolar cells shapes the encoding of artificial and natural stimuli},
  volume   = {109},
  issn     = {1097-4199},
  doi      = {10.1016/j.neuron.2021.03.015},
  abstract = {The retina dissects the visual scene into parallel information channels, which extract specific visual features through nonlinear processing. The first nonlinear stage is typically considered to occur at the output of bipolar cells, resulting from nonlinear transmitter release from synaptic terminals. In contrast, we show here that bipolar cells themselves can act as nonlinear processing elements at the level of their somatic membrane potential. Intracellular recordings from bipolar cells in the salamander retina revealed frequent nonlinear integration of visual signals within bipolar cell receptive field centers, affecting the encoding of artificial and natural stimuli. These nonlinearities provide sensitivity to spatial structure below the scale of bipolar cell receptive fields in both bipolar and downstream ganglion cells and appear to arise at the excitatory input into bipolar cells. Thus, our data suggest that nonlinear signal pooling starts earlier than previously thought: that is, at the input stage of bipolar cells.},
  language = {eng},
  number   = {10},
  journal  = {Neuron},
  author   = {Schreyer, Helene Marianne and Gollisch, Tim},
  month    = may,
  year     = {2021},
  pmid     = {33798407},
  pmcid    = {PMC8153253},
  keywords = {Animals, Coding, Feedback, Physiological, linear-nonlinear model, membrane potential recording, Membrane Potentials, Models, Neurological, natural stimuli, Nonlinear Dynamics, nonlinear encoding, Retinal bipolar cells, Retinal Bipolar Cells, Retinal Ganglion Cells, spatial integration, Urodela, Vision, Ocular, Visual Fields, read},
  pages    = {1692--1706.e8},
  file     = {Texte intégral:/home/baptistel/snap/zotero-snap/common/Zotero/storage/NC6NSUY5/Schreyer et Gollisch - 2021 - Nonlinear spatial integration in retinal bipolar c.pdf:application/pdf}
}

@article{schwartz_natural_2001,
  title     = {Natural signal statistics and sensory gain control},
  volume    = {4},
  copyright = {2001 Nature Publishing Group},
  issn      = {1546-1726},
  url       = {https://www.nature.com/articles/nn0801_819},
  doi       = {10.1038/90526},
  abstract  = {We describe a form of nonlinear decomposition that is well-suited for efficient encoding of natural signals. Signals are initially decomposed using a bank of linear filters. Each filter response is then rectified and divided by a weighted sum of rectified responses of neighboring filters. We show that this decomposition, with parameters optimized for the statistics of a generic ensemble of natural images or sounds, provides a good characterization of the nonlinear response properties of typical neurons in primary visual cortex or auditory nerve, respectively. These results suggest that nonlinear response properties of sensory neurons are not an accident of biological implementation, but have an important functional role.},
  language  = {en},
  number    = {8},
  urldate   = {2023-04-25},
  journal   = {Nature Neuroscience},
  author    = {Schwartz, Odelia and Simoncelli, Eero P.},
  month     = aug,
  year      = {2001},
  note      = {Number: 8
               Publisher: Nature Publishing Group},
  keywords  = {Natural Images, Animal Genetics and Genomics, Behavioral Sciences, Biological Techniques, Biomedicine, general, Neurobiology, Neurosciences, Read},
  pages     = {819--825},
  file      = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/APZDCSHN/Schwartz et Simoncelli - 2001 - Natural signal statistics and sensory gain control.pdf:application/pdf}
}

@article{soto_efficient_2020,
  title    = {Efficient {Coding} by {Midget} and {Parasol} {Ganglion} {Cells} in the {Human} {Retina}},
  volume   = {107},
  issn     = {0896-6273},
  url      = {https://www.sciencedirect.com/science/article/pii/S0896627320303998},
  doi      = {10.1016/j.neuron.2020.05.030},
  abstract = {In humans, midget and parasol ganglion cells account for most of the input from the eyes to the brain. Yet, how they encode visual information is unknown. Here, we perform large-scale multi-electrode array recordings from retinas of treatment-naive patients who underwent enucleation surgery for choroidal malignant melanomas. We identify robust differences in the function of midget and parasol ganglion cells, consistent asymmetries between their ON and OFF types (that signal light increments and decrements, respectively) and divergence in the function of human versus non-human primate retinas. Our computational analyses reveal that the receptive fields of human midget and parasol ganglion cells divide naturalistic movies into adjacent spatiotemporal frequency domains with equal stimulus power, while the asymmetric response functions of their ON and OFF types simultaneously maximize stimulus coverage and information transmission and minimize metabolic cost. Thus, midget and parasol ganglion cells in the human retina efficiently encode our visual environment.},
  language = {en},
  number   = {4},
  urldate  = {2023-07-12},
  journal  = {Neuron},
  author   = {Soto, Florentina and Hsiang, Jen-Chun and Rajagopal, Rithwick and Piggott, Kisha and Harocopos, George J. and Couch, Steven M. and Custer, Philip and Morgan, Josh L. and Kerschensteiner, Daniel},
  month    = aug,
  year     = {2020},
  keywords = {efficient coding, human neuroscience, LN model, metabolic cost, mutual information, naturalistic movies, parallel processing, redundancy, white noise},
  pages    = {656--666.e5},
  file     = {ScienceDirect Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/NSWDE9F3/Soto et al. - 2020 - Efficient Coding by Midget and Parasol Ganglion Ce.pdf:application/pdf;ScienceDirect Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/HMTAUWMQ/S0896627320303998.html:text/html}
}

@article{spoerer_recurrent_2020,
  title    = {Recurrent neural networks can explain flexible trading of speed and accuracy in biological vision},
  volume   = {16},
  issn     = {1553-7358},
  url      = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008215},
  doi      = {10.1371/journal.pcbi.1008215},
  abstract = {Deep feedforward neural network models of vision dominate in both computational neuroscience and engineering. The primate visual system, by contrast, contains abundant recurrent connections. Recurrent signal flow enables recycling of limited computational resources over time, and so might boost the performance of a physically finite brain or model. Here we show: (1) Recurrent convolutional neural network models outperform feedforward convolutional models matched in their number of parameters in large-scale visual recognition tasks on natural images. (2) Setting a confidence threshold, at which recurrent computations terminate and a decision is made, enables flexible trading of speed for accuracy. At a given confidence threshold, the model expends more time and energy on images that are harder to recognise, without requiring additional parameters for deeper computations. (3) The recurrent model’s reaction time for an image predicts the human reaction time for the same image better than several parameter-matched and state-of-the-art feedforward models. (4) Across confidence thresholds, the recurrent model emulates the behaviour of feedforward control models in that it achieves the same accuracy at approximately the same computational cost (mean number of floating-point operations). However, the recurrent model can be run longer (higher confidence threshold) and then outperforms parameter-matched feedforward comparison models. These results suggest that recurrent connectivity, a hallmark of biological visual systems, may be essential for understanding the accuracy, flexibility, and dynamics of human visual recognition.},
  language = {en},
  number   = {10},
  urldate  = {2023-05-04},
  journal  = {PLOS Computational Biology},
  author   = {Spoerer, Courtney J. and Kietzmann, Tim C. and Mehrer, Johannes and Charest, Ian and Kriegeskorte, Nikolaus},
  month    = oct,
  year     = {2020},
  note     = {Publisher: Public Library of Science},
  keywords = {read, DL, recurrence, Entropy, Feedforward neural networks, Graphs, Principal component analysis, Reaction time, Recurrent neural networks, Visual object recognition, Visual system},
  pages    = {e1008215},
  file     = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/H274NCJ2/Spoerer et al. - 2020 - Recurrent neural networks can explain flexible tra.pdf:application/pdf}
}

@misc{strubell_energy_2019,
  title     = {Energy and {Policy} {Considerations} for {Deep} {Learning} in {NLP}},
  url       = {http://arxiv.org/abs/1906.02243},
  doi       = {10.48550/arXiv.1906.02243},
  abstract  = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
  urldate   = {2023-05-15},
  publisher = {arXiv},
  author    = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  month     = jun,
  year      = {2019},
  note      = {arXiv:1906.02243 [cs]},
  keywords  = {DL, Ecology, Computer Science - Computation and Language},
  file      = {arXiv Fulltext PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/BWIGH72C/Strubell et al. - 2019 - Energy and Policy Considerations for Deep Learning.pdf:application/pdf;arXiv.org Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/I3L4IDK2/1906.html:text/html}
}

@article{tanaka_deep_2019,
  title      = {From deep learning to mechanistic understanding in neuroscience: the structure of retinal prediction},
  volume     = {32},
  issn       = {1049-5258},
  shorttitle = {From deep learning to mechanistic understanding in neuroscience},
  abstract   = {Recently, deep feedforward neural networks have achieved considerable success in modeling biological sensory processing, in terms of reproducing the input-output map of sensory neurons. However, such models raise profound questions about the very nature of explanation in neuroscience. Are we simply replacing one complex system (a biological circuit) with another (a deep network), without understanding either? Moreover, beyond neural representations, are the deep network's computational mechanisms for generating neural responses the same as those in the brain? Without a systematic approach to extracting and understanding computational mechanisms from deep neural network models, it can be difficult both to assess the degree of utility of deep learning approaches in neuroscience, and to extract experimentally testable hypotheses from deep networks. We develop such a systematic approach by combining dimensionality reduction and modern attribution methods for determining the relative importance of interneurons for specific visual computations. We apply this approach to deep network models of the retina, revealing a conceptual understanding of how the retina acts as a predictive feature extractor that signals deviations from expectations for diverse spatiotemporal stimuli. For each stimulus, our extracted computational mechanisms are consistent with prior scientific literature, and in one case yields a new mechanistic hypothesis. Thus overall, this work not only yields insights into the computational mechanisms underlying the striking predictive capabilities of the retina, but also places the framework of deep networks as neuroscientific models on firmer theoretical foundations, by providing a new roadmap to go beyond comparing neural representations to extracting and understand computational mechanisms.},
  language   = {eng},
  journal    = {Advances in Neural Information Processing Systems},
  author     = {Tanaka, Hidenori and Nayebi, Aran and Maheswaranathan, Niru and McIntosh, Lane and Baccus, Stephen A. and Ganguli, Surya},
  month      = dec,
  year       = {2019},
  pmid       = {35283616},
  pmcid      = {PMC8916592},
  keywords   = {read, DL},
  pages      = {8537--8547},
  file       = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/G29TYGF6/Tanaka et al. - 2019 - From deep learning to mechanistic understanding in.pdf:application/pdf}
}

@misc{themes_luminance_2016,
  title    = {Luminance {Range} for {Vision}},
  url      = {https://entokey.com/luminance-range-for-vision/},
  abstract = {(1) University of Sydney, Sydney, Australia   Overview 1. Mechanisms that broaden our luminance range for vision Our visual system can function over a wide range of light intensities, from sta…},
  language = {en-US},
  urldate  = {2023-08-22},
  journal  = {Ento Key},
  author   = {Themes, U. F. O.},
  month    = oct,
  year     = {2016},
  file     = {Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/9CUMF8X6/luminance-range-for-vision.html:text/html}
}

@article{tjostheim_cumulative_2019,
  title    = {Cumulative inhibition in neural networks},
  volume   = {20},
  issn     = {1612-4790},
  url      = {https://doi.org/10.1007/s10339-018-0888-z},
  doi      = {10.1007/s10339-018-0888-z},
  abstract = {We show how a multi-resolution network can model the development of acuity and coarse-to-fine processing in the mammalian visual cortex. The network adapts to input statistics in an unsupervised manner, and learns a coarse-to-fine representation by using cumulative inhibition of nodes within a network layer. We show that a system of such layers can represent input by hierarchically composing larger parts from smaller components. It can also model aspects of top-down processes, such as image regeneration.},
  language = {en},
  number   = {1},
  urldate  = {2023-05-12},
  journal  = {Cognitive Processing},
  author   = {Tjøstheim, Trond A. and Balkenius, Christian},
  month    = feb,
  year     = {2019},
  keywords = {Priority:2, Visual cortex, Acuity, Coarse-to-fine processing, Cortical microcolumn, Cumulative inhibition, Inhibition, Multi-resolution, Unsupervised learning},
  pages    = {87--102},
  file     = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/Q2EJMVQK/Tjøstheim and Balkenius - 2019 - Cumulative inhibition in neural networks.pdf:application/pdf}
}

@article{van_bergen_going_2020,
  series     = {Whole-brain interactions between neural circuits},
  title      = {Going in circles is the way forward: the role of recurrence in visual inference},
  volume     = {65},
  issn       = {0959-4388},
  shorttitle = {Going in circles is the way forward},
  url        = {https://www.sciencedirect.com/science/article/pii/S0959438820301768},
  doi        = {10.1016/j.conb.2020.11.009},
  abstract   = {Biological visual systems exhibit abundant recurrent connectivity. State-of-the-art neural network models for visual recognition, by contrast, rely heavily or exclusively on feedforward computation. Any finite-time recurrent neural network (RNN) can be unrolled along time to yield an equivalent feedforward neural network (FNN). This important insight suggests that computational neuroscientists may not need to engage recurrent computation, and that computer-vision engineers may be limiting themselves to a special case of FNN if they build recurrent models. Here we argue, to the contrary, that FNNs are a special case of RNNs and that computational neuroscientists and engineers should engage recurrence to understand how brains and machines can (1) achieve greater and more flexible computational depth (2) compress complex computations into limited hardware (3) integrate priors and priorities into visual inference through expectation and attention (4) exploit sequential dependencies in their data for better inference and prediction and (5) leverage the power of iterative computation.},
  language   = {en},
  urldate    = {2023-05-04},
  journal    = {Current Opinion in Neurobiology},
  author     = {van Bergen, Ruben S and Kriegeskorte, Nikolaus},
  month      = dec,
  year       = {2020},
  keywords   = {read, DL, recurrence},
  pages      = {176--193},
  file       = {ScienceDirect Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/FDG9IHYL/van Bergen and Kriegeskorte - 2020 - Going in circles is the way forward the role of r.pdf:application/pdf;ScienceDirect Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/9B2UMZ4L/S0959438820301768.html:text/html}
}

@article{wark_timescales_2009,
  title    = {Timescales of inference in visual adaptation},
  volume   = {61},
  issn     = {1097-4199},
  doi      = {10.1016/j.neuron.2009.01.019},
  abstract = {Adaptation is a hallmark of sensory function. Adapting optimally requires matching the dynamics of adaptation to those of changes in the stimulus distribution. Here we show that the dynamics of adaptation in the responses of mouse retinal ganglion cells depend on stimulus history. We hypothesized that the accumulation of evidence for a change in the stimulus distribution controls the dynamics of adaptation, and developed a model for adaptation as an ongoing inference problem. Guided by predictions of this model, we found that the dynamics of adaptation depend on the discriminability of the change in stimulus distribution and that the retina exploits information contained in properties of the stimulus beyond the mean and variance to adapt more quickly when possible.},
  language = {eng},
  number   = {5},
  journal  = {Neuron},
  author   = {Wark, Barry and Fairhall, Adrienne and Rieke, Fred},
  month    = mar,
  year     = {2009},
  pmid     = {19285471},
  pmcid    = {PMC2677143},
  keywords = {Retina, Animals, Models, Neurological, Nonlinear Dynamics, Retinal Ganglion Cells, read, Action Potentials, Adaptation, Physiological, Computer Simulation, Patch-Clamp Techniques, Photic Stimulation, Time Factors, Motion Perception, adaptation, Excitatory Postsynaptic Potentials, In Vitro Techniques, Luminescence, Mice, Mice, Inbred C57BL, Gain Control, mouse, Patch},
  pages    = {750--761},
  file     = {Texte intégral:/home/baptistel/snap/zotero-snap/common/Zotero/storage/6IV3G6WZ/Wark et al. - 2009 - Timescales of inference in visual adaptation.pdf:application/pdf}
}

@article{wassle_parallel_2004,
  title    = {Parallel processing in the mammalian retina},
  volume   = {5},
  issn     = {1471-003X},
  doi      = {10.1038/nrn1497},
  abstract = {Our eyes send different 'images' of the outside world to the brain - an image of contours (line drawing), a colour image (watercolour painting) or an image of moving objects (movie). This is commonly referred to as parallel processing, and starts as early as the first synapse of the retina, the cone pedicle. Here, the molecular composition of the transmitter receptors of the postsynaptic neurons defines which images are transferred to the inner retina. Within the second synaptic layer - the inner plexiform layer - circuits that involve complex inhibitory and excitatory interactions represent filters that select 'what the eye tells the brain'.},
  language = {eng},
  number   = {10},
  journal  = {Nature Reviews. Neuroscience},
  author   = {Wässle, Heinz},
  month    = oct,
  year     = {2004},
  pmid     = {15378035},
  keywords = {Retina, Neurons, Animals, Models, Neurological, Vision, Ocular, Synapses, Priority:1, Humans, Feedback, Nerve Net, Neural Networks, Computer, Retinal Cone Photoreceptor Cells, Retinal Rod Photoreceptor Cells, Visual Pathways},
  pages    = {747--757}
}

@misc{wpadmin-dproto_natural_nodate,
  title    = {Natural stimuli drive concerted nonlinear responses in populations of retinal ganglion cells},
  url      = {https://www.retina.uni-goettingen.de/publications/},
  abstract = {Preprints  
              
              Natural stimuli drive concerted nonlinear responses in populations of retinal ganglion cells
              Karamanlis D, Khani MH, Schreyer HM, Zapp SJ, Mietsch M, Gollisch T
              bioRxiv 2023.01.10.523412 (2023)
              link: https://www.biorxiv.org/content/10.1101/2023.01.10.523412v1 
              
              Filter-based models of suppression in retinal ganglion cells: comparison and generalization across species and stimuli
              Shahidi N, Rozenblit F, Khani MH, Schreyer HM,},
  language = {en-US},
  urldate  = {2023-05-17},
  journal  = {Sensory Processing in the Retina},
  author   = {{wpadmin-dproto}},
  keywords = {Natural Stimuli, Priority:1},
  file     = {Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/55295CVC/publications.html:text/html}
}

@article{yamins_using_2016,
  title    = {Using goal-driven deep learning models to understand sensory cortex},
  volume   = {19},
  issn     = {1097-6256, 1546-1726},
  url      = {https://www.nature.com/articles/nn.4244},
  doi      = {10.1038/nn.4244},
  language = {en},
  number   = {3},
  urldate  = {2023-05-22},
  journal  = {Nature Neuroscience},
  author   = {Yamins, Daniel L K and DiCarlo, James J},
  month    = mar,
  year     = {2016},
  keywords = {Priority:2, DL},
  pages    = {356--365},
  file     = {Yamins and DiCarlo - 2016 - Using goal-driven deep learning models to understa.pdf:/home/baptistel/snap/zotero-snap/common/Zotero/storage/BCHG56S8/Yamins and DiCarlo - 2016 - Using goal-driven deep learning models to understa.pdf:application/pdf}
}

@article{yuste_discovery_2015,
  title    = {The discovery of dendritic spines by {Cajal}},
  volume   = {9},
  issn     = {1662-5129},
  url      = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4404913/},
  doi      = {10.3389/fnana.2015.00018},
  abstract = {Dendritic spines were considered an artifact of the Golgi method until a brash Spanish histologist, Santiago Ramón y Cajal, bet his scientific career arguing that they were indeed real, correctly deducing their key role in mediating synaptic connectivity. This article reviews the historical context of the discovery of spines and the reasons behind Cajal's obsession with them, all the way till his deathbed.},
  urldate  = {2023-06-14},
  journal  = {Frontiers in Neuroanatomy},
  author   = {Yuste, Rafael},
  month    = apr,
  year     = {2015},
  pmid     = {25954162},
  pmcid    = {PMC4404913},
  pages    = {18},
  file     = {PubMed Central Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/TI8TLQP7/Yuste - 2015 - The discovery of dendritic spines by Cajal.pdf:application/pdf}
}

@article{zheng_unraveling_2021,
  title    = {Unraveling neural coding of dynamic natural visual scenes via convolutional recurrent neural networks},
  volume   = {2},
  issn     = {2666-3899},
  url      = {https://www.sciencedirect.com/science/article/pii/S2666389921002051},
  doi      = {10.1016/j.patter.2021.100350},
  abstract = {Traditional models of retinal system identification analyze the neural response to artificial stimuli using models consisting of predefined components. The model design is limited to prior knowledge, and the artificial stimuli are too simple to be compared with stimuli processed by the retina. To fill in this gap with an explainable model that reveals how a population of neurons work together to encode the larger field of natural scenes, here we used a deep-learning model for identifying the computational elements of the retinal circuit that contribute to learning the dynamics of natural scenes. Experimental results verify that the recurrent connection plays a key role in encoding complex dynamic visual scenes while learning biological computational underpinnings of the retinal circuit. In addition, the proposed models reveal both the shapes and the locations of the spatiotemporal receptive fields of ganglion cells.},
  language = {en},
  number   = {10},
  urldate  = {2023-07-13},
  journal  = {Patterns},
  author   = {Zheng, Yajing and Jia, Shanshan and Yu, Zhaofei and Liu, Jian K. and Huang, Tiejun},
  month    = oct,
  year     = {2021},
  keywords = {Priority:0, retina, convolutional neural network, neural coding, recurrent neural network, video analysis, visual coding},
  pages    = {100350},
  file     = {ScienceDirect Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/3VTH5V49/Zheng et al. - 2021 - Unraveling neural coding of dynamic natural visual.pdf:application/pdf;ScienceDirect Snapshot:/home/baptistel/snap/zotero-snap/common/Zotero/storage/EKZGYT8A/S2666389921002051.html:text/html}
}

@article{zhuang_suppression_2023,
  title      = {Suppression helps: {Lateral} {Inhibition}-inspired {Convolutional} {Neural} {Network} for {Image} {Classification}},
  shorttitle = {Suppression helps},
  url        = {https://openreview.net/forum?id=N3kGYG3ZcTi},
  abstract   = {Convolutional neural networks (CNNs) have become powerful and popular tools since deep learning emerged for image classification in the computer vision field. For better recognition, the dimensions of depth and width have been explored, leading to convolutional neural networks with more layers and more channels. In addition to these factors, neurobiology also suggests the widely existing lateral inhibition (e.g., Mach band effect), which increases the contrast of nearby neuron excitation in the lateral direction, to help recognition. However, such an important mechanism has not been well explored in modern convolutional neural networks. In this paper, we explicitly explore the filter dimension in the lateral direction and propose our lateral inhibition-inspired (LI) design. Our naive design incorporates the low-pass filter, while eliminating the central weight to mimic the inhibition strength decay. The inhibition value is computed from the filtering result of the input, with a simple learnable weight parameter per channel for multiplication to decide the strength. Then the inhibition value is subtracted from the input as suppression, which could increase the contrast to help recognition. We also suggest an alternative using depthwise convolution, as a general form. Our design could work on both the plain convolution and the convolutional block with residual connection, while being compatible with existing modules. Without any channel attention along the channel dimension, the preliminary results demonstrate an absolute improvement of 3.68{\textbackslash}\% and 0.69{\textbackslash}\% over AlexNet and ResNet-18, respectively, in the ImageNet data set, with little increase in parameters, indicating the merits of our design to help feature learning for image classification.},
  language   = {en},
  urldate    = {2023-05-12},
  author     = {Zhuang, Chengyuan and Yuan, Xiaohui and Guo, Xuan},
  month      = feb,
  year       = {2023},
  keywords   = {DL, priority:1},
  file       = {Full Text PDF:/home/baptistel/snap/zotero-snap/common/Zotero/storage/6FTTQTWG/Zhuang et al. - 2023 - Suppression helps Lateral Inhibition-inspired Con.pdf:application/pdf}
}
